#!/usr/bin/env python3
"""
TempoVLM è¦–è¦ºåŒ–å±•ç¤ºè…³æœ¬
======================

ç”Ÿæˆä¸‰ç¨®è¦–è¦ºåŒ–å±•ç¤ºï¼š
1. æ™‚åºç©©å®šæ€§ - Split Screen é®æ“‹æ¸¬è©¦å½±ç‰‡
2. æ·±åº¦æ„ŸçŸ¥ - Depth Radar å„€è¡¨æ¿
3. é‹å‹•æ„ŸçŸ¥ - Real-time Trajectory Plot

è¼¸å‡ºï¼š
- å°æ¯”å½±ç‰‡
- å„€è¡¨æ¿æˆªåœ–
- è»Œè·¡å‹•ç•«
"""

import os
import sys
import json
import torch
import torch.nn.functional as F
import numpy as np
from pathlib import Path
from PIL import Image, ImageDraw, ImageFont
from tqdm import tqdm
import matplotlib.pyplot as plt
import matplotlib
matplotlib.use('Agg')
from matplotlib.patches import Circle, Wedge
from matplotlib.collections import PatchCollection
import cv2
import argparse
from collections import deque

# Qwen2-VL
from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
from qwen_vl_utils import process_vision_info


class TempoVLMVisualizer:
    """TempoVLM è¦–è¦ºåŒ–å™¨"""
    
    def __init__(self, unified_model_path, device='cuda'):
        self.device = device
        
        print("=" * 70)
        print("ğŸ¨ TempoVLM Visualizer")
        print("=" * 70)
        
        # è¼‰å…¥æ¨¡å‹
        print("\nğŸ“¦ è¼‰å…¥æ¨¡å‹...")
        self.processor = AutoProcessor.from_pretrained(
            "Qwen/Qwen2-VL-2B-Instruct",
            trust_remote_code=True
        )
        self.base_model = Qwen2VLForConditionalGeneration.from_pretrained(
            "Qwen/Qwen2-VL-2B-Instruct",
            torch_dtype=torch.float16,
            device_map=device,
            trust_remote_code=True
        ).eval()
        
        self._load_unified_model(unified_model_path)
        print("âœ… æ¨¡å‹è¼‰å…¥å®Œæˆï¼")
    
    def _load_unified_model(self, model_path):
        """è¼‰å…¥ Unified Model"""
        from models_unified import UnifiedTempoVLM
        
        checkpoint = torch.load(model_path, map_location=self.device, weights_only=False)
        state_dict = checkpoint['model_state_dict'] if 'model_state_dict' in checkpoint else checkpoint
        
        if 'shared_encoder.0.weight' in state_dict:
            hidden_dim = state_dict['shared_encoder.0.weight'].shape[0]
        else:
            hidden_dim = 768
        
        self.unified_model = UnifiedTempoVLM(hidden_dim=hidden_dim).to(self.device)
        
        if 'model_state_dict' in checkpoint:
            self.unified_model.load_state_dict(checkpoint['model_state_dict'])
        else:
            self.unified_model.load_state_dict(checkpoint)
        
        self.unified_model.eval()
        self.unified_model.half()
    
    def extract_features(self, image):
        """æå–ç‰¹å¾µ"""
        messages = [{
            "role": "user",
            "content": [
                {"type": "image", "image": image},
                {"type": "text", "text": "Describe."}
            ]
        }]
        
        text = self.processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        image_inputs, video_inputs = process_vision_info(messages)
        inputs = self.processor(
            text=[text],
            images=image_inputs,
            videos=video_inputs,
            padding=True,
            return_tensors="pt"
        ).to(self.device)
        
        with torch.no_grad():
            outputs = self.base_model(
                **inputs,
                output_hidden_states=True,
                return_dict=True
            )
            hidden_states = outputs.hidden_states[-1]
            features = hidden_states.mean(dim=1)
        
        return features.half()
    
    # ========== 1. æ™‚åºç©©å®šæ€§è¦–è¦ºåŒ– ==========
    
    def visualize_temporal_consistency(self, scene_dir, output_path, max_frames=60):
        """
        ç”Ÿæˆæ™‚åºä¸€è‡´æ€§å°æ¯”å½±ç‰‡
        - å·¦å´: Base Model ç‰¹å¾µç›¸ä¼¼åº¦æ›²ç·š
        - å³å´: Unified Model ç‰¹å¾µç›¸ä¼¼åº¦æ›²ç·š
        - åº•éƒ¨: ç›¸ä¼¼åº¦éš¨æ™‚é–“è®ŠåŒ–çš„åœ–
        """
        print("\nğŸ¬ ç”Ÿæˆæ™‚åºä¸€è‡´æ€§å°æ¯”å½±ç‰‡...")
        
        color_dir = scene_dir / 'color'
        frame_files = sorted(color_dir.glob('*.jpg'))[:max_frames]
        
        if len(frame_files) < 10:
            print("âŒ å¹€æ•¸ä¸è¶³")
            return
        
        # æå–ç‰¹å¾µ
        print("  æå–ç‰¹å¾µ...")
        base_features = []
        for f in tqdm(frame_files, desc="  Base"):
            img = Image.open(f).convert('RGB')
            feat = self.extract_features(img)
            base_features.append(feat)
        
        unified_features = [base_features[0]]
        for i in range(1, len(base_features)):
            with torch.no_grad():
                outputs = self.unified_model(base_features[i], base_features[i-1], tasks=['temporal'])
                unified_features.append(outputs['temporal'])
        
        # è¨ˆç®—ç›¸ä¼¼åº¦
        base_sims = [1.0]  # ç¬¬ä¸€å¹€èˆ‡è‡ªå·±çš„ç›¸ä¼¼åº¦
        unified_sims = [1.0]
        for i in range(1, len(base_features)):
            base_sim = F.cosine_similarity(base_features[i], base_features[i-1], dim=-1).item()
            unified_sim = F.cosine_similarity(unified_features[i], unified_features[i-1], dim=-1).item()
            base_sims.append(base_sim)
            unified_sims.append(unified_sim)
        
        # ç”Ÿæˆå½±ç‰‡
        print("  ç”Ÿæˆå½±ç‰‡...")
        
        # å½±ç‰‡åƒæ•¸
        frame_width = 1280
        frame_height = 720
        fps = 10
        
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(str(output_path), fourcc, fps, (frame_width, frame_height))
        
        for i, frame_file in enumerate(tqdm(frame_files, desc="  å¯«å…¥å¹€")):
            # è®€å–åŸå§‹å¹€
            img = cv2.imread(str(frame_file))
            img = cv2.resize(img, (640, 480))
            
            # å‰µå»ºç•«å¸ƒ
            canvas = np.zeros((frame_height, frame_width, 3), dtype=np.uint8)
            canvas[:] = (30, 30, 30)  # æ·±ç°èƒŒæ™¯
            
            # æ”¾ç½®åŸå§‹åœ–ç‰‡
            canvas[20:500, 20:660] = img
            
            # ç¹ªè£½ç›¸ä¼¼åº¦æ›²ç·š
            fig, ax = plt.subplots(figsize=(6, 3), facecolor='#1e1e1e')
            ax.set_facecolor('#1e1e1e')
            
            # ç¹ªè£½åˆ°ç•¶å‰å¹€ç‚ºæ­¢çš„æ›²ç·š
            x = np.arange(i + 1)
            ax.plot(x, base_sims[:i+1], 'r-', label='Base Model', linewidth=2)
            ax.plot(x, unified_sims[:i+1], 'g-', label='Unified Model', linewidth=2)
            
            ax.set_xlim(0, len(frame_files))
            ax.set_ylim(0.8, 1.0)
            ax.set_xlabel('Frame', color='white')
            ax.set_ylabel('Cosine Similarity', color='white')
            ax.set_title('Temporal Feature Consistency', color='white')
            ax.legend(loc='lower left', facecolor='#2e2e2e', edgecolor='white', labelcolor='white')
            ax.tick_params(colors='white')
            ax.grid(True, alpha=0.3)
            
            for spine in ax.spines.values():
                spine.set_edgecolor('white')
            
            # ä¿å­˜åœ–è¡¨ç‚ºåœ–ç‰‡
            fig.tight_layout()
            fig.canvas.draw()
            
            # ä½¿ç”¨æ–°çš„ API
            plot_img = np.array(fig.canvas.buffer_rgba())[:, :, :3]  # RGBA -> RGB
            plot_img = cv2.cvtColor(plot_img, cv2.COLOR_RGB2BGR)
            plot_img = cv2.resize(plot_img, (600, 200))
            plt.close(fig)
            
            # æ”¾ç½®åœ–è¡¨
            canvas[510:710, 20:620] = plot_img
            
            # ç¹ªè£½ç•¶å‰å¹€çš„æŒ‡æ¨™
            current_base_sim = base_sims[i]
            current_unified_sim = unified_sims[i]
            
            # å³å´é¢æ¿
            panel_x = 680
            cv2.putText(canvas, f'Frame: {i+1}/{len(frame_files)}', (panel_x, 50),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)
            
            cv2.putText(canvas, 'Current Similarity:', (panel_x, 100),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
            
            cv2.putText(canvas, f'Base:    {current_base_sim:.4f}', (panel_x, 140),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (100, 100, 255), 2)
            
            cv2.putText(canvas, f'Unified: {current_unified_sim:.4f}', (panel_x, 180),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (100, 255, 100), 2)
            
            # æ”¹å–„æŒ‡ç¤º
            improve = (current_unified_sim - current_base_sim) / current_base_sim * 100
            color = (100, 255, 100) if improve > 0 else (100, 100, 255)
            cv2.putText(canvas, f'Improvement: {improve:+.2f}%', (panel_x, 230),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)
            
            # å¹³å‡çµ±è¨ˆ
            cv2.putText(canvas, 'Average (so far):', (panel_x, 300),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
            cv2.putText(canvas, f'Base:    {np.mean(base_sims[:i+1]):.4f}', (panel_x, 340),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (100, 100, 255), 2)
            cv2.putText(canvas, f'Unified: {np.mean(unified_sims[:i+1]):.4f}', (panel_x, 380),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (100, 255, 100), 2)
            
            # ç¹ªè£½ç›¸ä¼¼åº¦æ¢
            bar_y = 450
            bar_width = 400
            bar_height = 30
            
            # Base Model æ¢
            cv2.putText(canvas, 'Base', (panel_x, bar_y - 10),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
            cv2.rectangle(canvas, (panel_x, bar_y), (panel_x + bar_width, bar_y + bar_height), (50, 50, 50), -1)
            base_width = int(bar_width * (current_base_sim - 0.8) / 0.2)  # 0.8-1.0 ç¯„åœ
            cv2.rectangle(canvas, (panel_x, bar_y), (panel_x + base_width, bar_y + bar_height), (100, 100, 255), -1)
            
            # Unified Model æ¢
            bar_y2 = bar_y + 50
            cv2.putText(canvas, 'Unified', (panel_x, bar_y2 - 10),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
            cv2.rectangle(canvas, (panel_x, bar_y2), (panel_x + bar_width, bar_y2 + bar_height), (50, 50, 50), -1)
            unified_width = int(bar_width * (current_unified_sim - 0.8) / 0.2)
            cv2.rectangle(canvas, (panel_x, bar_y2), (panel_x + unified_width, bar_y2 + bar_height), (100, 255, 100), -1)
            
            # æ¨™é¡Œ
            cv2.putText(canvas, 'TempoVLM: Temporal Consistency Demo', (20, frame_height - 20),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (150, 150, 150), 1)
            
            out.write(canvas)
        
        out.release()
        print(f"âœ… å½±ç‰‡å·²ä¿å­˜: {output_path}")
    
    # ========== 2. æ·±åº¦æ„ŸçŸ¥è¦–è¦ºåŒ– ==========
    
    def visualize_depth_radar(self, scene_dir, output_path, max_frames=60):
        """
        ç”Ÿæˆæ·±åº¦æ’åºèƒ½åŠ›å±•ç¤ºå½±ç‰‡
        
        æ­£ç¢ºçš„æ¸¬è©¦æ–¹å¼ï¼š
        1. è£åˆ‡ç•«é¢ä¸­çš„ä¸åŒå€åŸŸ
        2. å°æ¯å€‹å€åŸŸåˆ†åˆ¥æå– Qwen2-VL ç‰¹å¾µ
        3. ç”¨çœŸå¯¦çš„å€åŸŸç‰¹å¾µåšæ·±åº¦æ’åºé æ¸¬
        """
        print("\nğŸ¬ ç”Ÿæˆæ·±åº¦æ’åºèƒ½åŠ›å±•ç¤º...")
        
        color_dir = scene_dir / 'color'
        depth_dir = scene_dir / 'depth'
        
        frame_files = sorted(color_dir.glob('*.jpg'))[:max_frames]
        
        if len(frame_files) < 10:
            print("âŒ å¹€æ•¸ä¸è¶³")
            return
        
        # å½±ç‰‡åƒæ•¸
        frame_width = 1280
        frame_height = 720
        fps = 10
        
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(str(output_path), fourcc, fps, (frame_width, frame_height))
        
        # çµ±è¨ˆ
        correct_predictions = 0
        total_predictions = 0
        accuracy_history = deque(maxlen=30)
        
        for i, frame_file in enumerate(tqdm(frame_files, desc="  ç”Ÿæˆå¹€")):
            # è®€å–åœ–åƒå’Œæ·±åº¦
            img_pil = Image.open(frame_file).convert('RGB')
            img_cv = cv2.imread(str(frame_file))
            img_resized = cv2.resize(img_cv, (640, 360))
            
            depth_file = depth_dir / (frame_file.stem + '.png')
            if depth_file.exists():
                depth_raw = cv2.imread(str(depth_file), cv2.IMREAD_UNCHANGED)
                depth = depth_raw.astype(np.float32) / 1000.0
            else:
                depth = np.ones((480, 640)) * 5.0
            
            depth_resized = cv2.resize(depth, (640, 360))
            
            # å®šç¾©ä¸‰å€‹å€åŸŸï¼ˆåœ¨åŸåœ–ä¸Šè£åˆ‡ï¼‰
            img_w, img_h = img_pil.size
            region_width = img_w // 3
            region_height = img_h // 2  # å–ä¸­é–“ä¸€åŠé«˜åº¦
            y_start = img_h // 4
            
            # è£åˆ‡ä¸‰å€‹å€åŸŸçš„åœ–ç‰‡
            region_crops = {
                'left': img_pil.crop((0, y_start, region_width, y_start + region_height)),
                'center': img_pil.crop((region_width, y_start, 2*region_width, y_start + region_height)),
                'right': img_pil.crop((2*region_width, y_start, img_w, y_start + region_height)),
            }
            
            # è¨ˆç®—ä¸‰å€‹å€åŸŸçš„ GT æ·±åº¦
            h, w = depth_resized.shape
            gt_depths = {}
            depth_regions = {
                'left': depth_resized[h//4:3*h//4, :w//3],
                'center': depth_resized[h//4:3*h//4, w//3:2*w//3],
                'right': depth_resized[h//4:3*h//4, 2*w//3:],
            }
            
            for name, region in depth_regions.items():
                valid = region[(region > 0.1) & (region < 10)]
                gt_depths[name] = valid.mean() if len(valid) > 0 else 5.0
            
            # æå–ä¸‰å€‹å€åŸŸçš„ç‰¹å¾µ
            test_results = []
            
            if self.unified_model is not None:
                with torch.no_grad():
                    # å°æ¯å€‹å€åŸŸåˆ†åˆ¥æå–ç‰¹å¾µ
                    region_features = {}
                    for name, crop in region_crops.items():
                        # èª¿æ•´å¤§å°ç¢ºä¿æ¨¡å‹èƒ½è™•ç†
                        crop_resized = crop.resize((224, 224))
                        feat = self.extract_features(crop_resized)
                        region_features[name] = feat
                    
                    # ç·¨ç¢¼ç‰¹å¾µ
                    left_enc = self.unified_model.shared_encoder(region_features['left'].half())
                    center_enc = self.unified_model.shared_encoder(region_features['center'].half())
                    right_enc = self.unified_model.shared_encoder(region_features['right'].half())
                    
                    # GT ç­”æ¡ˆ
                    gt_order_lc = 0 if gt_depths['left'] < gt_depths['center'] else 1
                    gt_order_cr = 0 if gt_depths['center'] < gt_depths['right'] else 1
                    gt_order_lr = 0 if gt_depths['left'] < gt_depths['right'] else 1
                    
                    # é æ¸¬ Left vs Center
                    combined_lc = torch.cat([left_enc, center_enc], dim=-1)
                    logits_lc = self.unified_model.depth_order_head(combined_lc)
                    pred_lc = torch.argmax(logits_lc, dim=-1).item()
                    conf_lc = torch.softmax(logits_lc, dim=-1).max().item()
                    correct_lc = (pred_lc == gt_order_lc)
                    
                    # é æ¸¬ Center vs Right
                    combined_cr = torch.cat([center_enc, right_enc], dim=-1)
                    logits_cr = self.unified_model.depth_order_head(combined_cr)
                    pred_cr = torch.argmax(logits_cr, dim=-1).item()
                    conf_cr = torch.softmax(logits_cr, dim=-1).max().item()
                    correct_cr = (pred_cr == gt_order_cr)
                    
                    # é æ¸¬ Left vs Right
                    combined_lr = torch.cat([left_enc, right_enc], dim=-1)
                    logits_lr = self.unified_model.depth_order_head(combined_lr)
                    pred_lr = torch.argmax(logits_lr, dim=-1).item()
                    conf_lr = torch.softmax(logits_lr, dim=-1).max().item()
                    correct_lr = (pred_lr == gt_order_lr)
                    
                    test_results = [
                        ('L vs C', gt_order_lc, pred_lc, correct_lc, conf_lc, gt_depths['left'], gt_depths['center']),
                        ('C vs R', gt_order_cr, pred_cr, correct_cr, conf_cr, gt_depths['center'], gt_depths['right']),
                        ('L vs R', gt_order_lr, pred_lr, correct_lr, conf_lr, gt_depths['left'], gt_depths['right']),
                    ]
                    
                    # æ›´æ–°çµ±è¨ˆ
                    for result in test_results:
                        total_predictions += 1
                        if result[3]:  # correct
                            correct_predictions += 1
                    
                    if total_predictions > 0:
                        accuracy_history.append(correct_predictions / total_predictions)
            
            # å‰µå»ºç•«å¸ƒ
            canvas = np.zeros((frame_height, frame_width, 3), dtype=np.uint8)
            canvas[:] = (30, 30, 30)
            
            # ========== å·¦ä¸Š: åŸå§‹åœ– + å€åŸŸæ¨™è¨˜ ==========
            img_with_regions = img_resized.copy()
            
            # ç•«å€åŸŸåˆ†å‰²ç·šå’Œæ¡†
            h_vis, w_vis = img_with_regions.shape[:2]
            y1, y2 = h_vis//4, 3*h_vis//4
            
            # å·¦å€åŸŸæ¡† (ç´…)
            cv2.rectangle(img_with_regions, (0, y1), (w_vis//3, y2), (100, 100, 255), 2)
            cv2.putText(img_with_regions, f'L:{gt_depths["left"]:.1f}m', (5, y1 + 20),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (100, 100, 255), 2)
            
            # ä¸­å€åŸŸæ¡† (ç¶ )
            cv2.rectangle(img_with_regions, (w_vis//3, y1), (2*w_vis//3, y2), (100, 255, 100), 2)
            cv2.putText(img_with_regions, f'C:{gt_depths["center"]:.1f}m', (w_vis//3 + 5, y1 + 20),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (100, 255, 100), 2)
            
            # å³å€åŸŸæ¡† (è—)
            cv2.rectangle(img_with_regions, (2*w_vis//3, y1), (w_vis, y2), (255, 100, 100), 2)
            cv2.putText(img_with_regions, f'R:{gt_depths["right"]:.1f}m', (2*w_vis//3 + 5, y1 + 20),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 100, 100), 2)
            
            canvas[20:380, 20:660] = img_with_regions
            cv2.putText(canvas, 'RGB + Region Crops (for feature extraction)', (25, 400),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200, 200, 200), 1)
            
            # ========== å³ä¸Š: æ·±åº¦åœ–è¦–è¦ºåŒ– ==========
            depth_vis = cv2.applyColorMap(
                (np.clip(depth_resized / 5.0, 0, 1) * 255).astype(np.uint8),
                cv2.COLORMAP_JET
            )
            # ä¹Ÿç•«ä¸Šå€åŸŸæ¡†
            cv2.rectangle(depth_vis, (0, h//4), (w//3, 3*h//4), (255, 255, 255), 1)
            cv2.rectangle(depth_vis, (w//3, h//4), (2*w//3, 3*h//4), (255, 255, 255), 1)
            cv2.rectangle(depth_vis, (2*w//3, h//4), (w, 3*h//4), (255, 255, 255), 1)
            
            canvas[20:380, 680:1260] = cv2.resize(depth_vis, (580, 360))
            cv2.putText(canvas, 'GT Depth Map (white boxes = compared regions)', (685, 400),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200, 200, 200), 1)
            
            # ========== ä¸‹åŠéƒ¨: æ·±åº¦æ’åºæ¸¬è©¦çµæœ ==========
            panel_y = 430
            
            cv2.putText(canvas, 'Depth Ordering Test (Real Region Features)', (20, panel_y),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)
            cv2.putText(canvas, f'Frame: {i+1}/{len(frame_files)}', (550, panel_y),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (200, 200, 200), 1)
            
            # æ¸¬è©¦çµæœè¡¨æ ¼
            table_y = panel_y + 40
            headers = ['Test', 'Depths', 'GT', 'Pred', 'Result', 'Conf']
            col_x = [20, 100, 220, 320, 420, 490]
            
            for j, hdr in enumerate(headers):
                cv2.putText(canvas, hdr, (col_x[j], table_y),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (150, 150, 150), 1)
            
            for j, result in enumerate(test_results):
                test_name, gt, pred, correct, conf, depth_a, depth_b = result
                row_y = table_y + 30 + j * 35
                
                # Test name
                cv2.putText(canvas, test_name, (col_x[0], row_y),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)
                
                # Actual depths
                cv2.putText(canvas, f'{depth_a:.1f}m vs {depth_b:.1f}m', (col_x[1], row_y),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.45, (200, 200, 200), 1)
                
                # GT (A closer / B closer)
                gt_text = 'A closer' if gt == 0 else 'B closer'
                cv2.putText(canvas, gt_text, (col_x[2], row_y),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (100, 255, 100), 1)
                
                # Pred
                pred_text = 'A closer' if pred == 0 else 'B closer'
                cv2.putText(canvas, pred_text, (col_x[3], row_y),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (100, 100, 255), 1)
                
                # Result
                result_text = 'OK' if correct else 'X'
                result_color = (0, 255, 0) if correct else (0, 0, 255)
                cv2.putText(canvas, result_text, (col_x[4], row_y),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, result_color, 2)
                
                # Confidence
                cv2.putText(canvas, f'{conf:.2f}', (col_x[5], row_y),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200, 200, 200), 1)
            
            # ========== ç´¯ç©æº–ç¢ºç‡ ==========
            stats_x = 600
            cv2.putText(canvas, 'Cumulative Stats:', (stats_x, panel_y + 40),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)
            
            if total_predictions > 0:
                acc = correct_predictions / total_predictions
                acc_color = (0, 255, 0) if acc > 0.6 else (0, 165, 255) if acc > 0.4 else (0, 0, 255)
                cv2.putText(canvas, f'Accuracy: {acc:.1%}', (stats_x, panel_y + 75),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, acc_color, 2)
                cv2.putText(canvas, f'({correct_predictions}/{total_predictions})', (stats_x + 150, panel_y + 75),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200, 200, 200), 1)
                
                # èˆ‡éš¨æ©ŸåŸºç·šæ¯”è¼ƒ
                random_baseline = 0.5
                improvement = acc - random_baseline
                imp_color = (0, 255, 0) if improvement > 0 else (0, 0, 255)
                cv2.putText(canvas, f'vs Random: {improvement:+.1%}', (stats_x, panel_y + 105),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, imp_color, 1)
            
            # ========== æº–ç¢ºç‡æ›²ç·š ==========
            if len(accuracy_history) > 1:
                graph_x = 600
                graph_y = panel_y + 130
                graph_w = 350
                graph_h = 80
                
                cv2.rectangle(canvas, (graph_x, graph_y), (graph_x + graph_w, graph_y + graph_h),
                             (50, 50, 50), -1)
                
                # 50% åŸºæº–ç·š
                baseline_y = graph_y + graph_h // 2
                cv2.line(canvas, (graph_x, baseline_y), (graph_x + graph_w, baseline_y),
                        (100, 100, 100), 1)
                cv2.putText(canvas, '50%', (graph_x - 30, baseline_y + 5),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.35, (100, 100, 100), 1)
                
                # ç¹ªè£½æ›²ç·š
                points = []
                for j, acc in enumerate(accuracy_history):
                    x = graph_x + int(j * graph_w / 30)
                    y = graph_y + graph_h - int(acc * graph_h)
                    points.append((x, y))
                
                if len(points) > 1:
                    for j in range(len(points) - 1):
                        color = (0, 255, 0) if list(accuracy_history)[j] > 0.5 else (0, 0, 255)
                        cv2.line(canvas, points[j], points[j+1], color, 2)
                
                cv2.putText(canvas, 'Accuracy History (green=better than random)', (graph_x, graph_y - 5),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.4, (200, 200, 200), 1)
            
            # ========== èªªæ˜æ–‡å­— ==========
            note_y = 680
            cv2.putText(canvas, 'Method: Crop L/C/R regions -> Extract Qwen2-VL features -> depth_order_head predicts which is closer',
                       (20, note_y), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (150, 150, 150), 1)
            
            # æ¨™é¡Œ
            cv2.putText(canvas, f'TempoVLM: Depth Ordering Demo - Frame {i+1}/{len(frame_files)}',
                       (20, frame_height - 10),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (100, 100, 100), 1)
            
            out.write(canvas)
        
        out.release()
        
        # è¼¸å‡ºçµ±è¨ˆ
        if total_predictions > 0:
            final_acc = correct_predictions / total_predictions
            print(f"  ğŸ“Š æœ€çµ‚æ·±åº¦æ’åºæº–ç¢ºç‡: {final_acc:.1%} ({correct_predictions}/{total_predictions})")
        
        print(f"âœ… å½±ç‰‡å·²ä¿å­˜: {output_path}")
    
    # ========== 2.5 æ·±åº¦å›æ­¸è¦–è¦ºåŒ– (éœ€è¦ depth_regression è¨“ç·´) ==========
    
    def visualize_depth_regression(self, scene_dir, output_path, max_frames=60):
        """
        ç”Ÿæˆæ·±åº¦å›æ­¸é æ¸¬ vs Ground Truth çš„æ¯”è¼ƒå½±ç‰‡
        
        é¡¯ç¤ºå…§å®¹:
        1. åŸå§‹ RGB åœ–åƒ
        2. GT æ·±åº¦åœ– (ç†±åŠ›åœ–)
        3. ä¸‰å€‹å€åŸŸ (å·¦/ä¸­/å³) çš„é æ¸¬æ·±åº¦ vs GT æ·±åº¦
        4. é æ¸¬èª¤å·®æ›²ç·š
        """
        print("\nğŸ¬ ç”Ÿæˆæ·±åº¦å›æ­¸æ¯”è¼ƒå½±ç‰‡...")
        
        # æª¢æŸ¥æ¨¡å‹æ˜¯å¦æ”¯æ´ depth_regression
        if not hasattr(self.unified_model, 'depth_regression_head'):
            print("âŒ æ¨¡å‹æœªåŒ…å« depth_regression_headï¼Œéœ€è¦ç”¨ depth_regression ä»»å‹™è¨“ç·´")
            return
        
        color_dir = scene_dir / 'color'
        depth_dir = scene_dir / 'depth'
        
        frame_files = sorted(color_dir.glob('*.jpg'))[:max_frames]
        
        if len(frame_files) < 10:
            print("âŒ å¹€æ•¸ä¸è¶³")
            return
        
        # å½±ç‰‡åƒæ•¸
        frame_width = 1280
        frame_height = 720
        fps = 10
        
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(str(output_path), fourcc, fps, (frame_width, frame_height))
        
        # çµ±è¨ˆ
        errors_left = []
        errors_center = []
        errors_right = []
        all_preds = {'left': [], 'center': [], 'right': []}
        all_gts = {'left': [], 'center': [], 'right': []}
        
        for i, frame_file in enumerate(tqdm(frame_files, desc="  ç”Ÿæˆå¹€")):
            # è®€å–åœ–åƒ
            img_pil = Image.open(frame_file).convert('RGB')
            img_cv = cv2.imread(str(frame_file))
            img_resized = cv2.resize(img_cv, (640, 360))
            
            # è®€å–æ·±åº¦
            depth_file = depth_dir / (frame_file.stem + '.png')
            if depth_file.exists():
                depth_raw = cv2.imread(str(depth_file), cv2.IMREAD_UNCHANGED)
                depth = depth_raw.astype(np.float32) / 1000.0  # mm to m
            else:
                depth = np.ones((480, 640)) * 3.0
            
            depth_resized = cv2.resize(depth, (640, 360))
            
            # å®šç¾©ä¸‰å€‹å€åŸŸ
            img_w, img_h = img_pil.size
            region_width = img_w // 3
            region_height = img_h // 2
            y_start = img_h // 4
            
            regions = {
                'left': img_pil.crop((0, y_start, region_width, y_start + region_height)),
                'center': img_pil.crop((region_width, y_start, 2*region_width, y_start + region_height)),
                'right': img_pil.crop((2*region_width, y_start, img_w, y_start + region_height)),
            }
            
            # è¨ˆç®— GT æ·±åº¦
            h, w = depth_resized.shape
            depth_regions = {
                'left': depth_resized[h//4:3*h//4, :w//3],
                'center': depth_resized[h//4:3*h//4, w//3:2*w//3],
                'right': depth_resized[h//4:3*h//4, 2*w//3:],
            }
            
            gt_depths = {}
            for name, region in depth_regions.items():
                valid = region[(region > 0.1) & (region < 10)]
                if len(valid) > 0:
                    gt_depths[name] = valid.mean()
                else:
                    gt_depths[name] = 3.0
            
            # é æ¸¬æ·±åº¦
            pred_depths = {}
            with torch.no_grad():
                for name, crop in regions.items():
                    crop_resized = crop.resize((224, 224))
                    feat = self.extract_features(crop_resized)
                    feat_enc = self.unified_model.shared_encoder(feat.half())
                    
                    # æ·±åº¦å›æ­¸é æ¸¬ (è¼¸å‡º 0-1ï¼Œéœ€è¦åæ­£è¦åŒ–)
                    pred_norm = self.unified_model.depth_regression_head(feat_enc)
                    pred_norm = pred_norm.squeeze().item()
                    
                    # åæ­£è¦åŒ–: normalized = (depth - 0.5) / 4.5
                    # depth = normalized * 4.5 + 0.5
                    pred_depth = pred_norm * 4.5 + 0.5
                    pred_depth = max(0.5, min(5.0, pred_depth))  # Clamp to valid range
                    
                    pred_depths[name] = pred_depth
            
            # è¨ˆç®—èª¤å·®
            for name in ['left', 'center', 'right']:
                error = abs(pred_depths[name] - gt_depths[name])
                if name == 'left':
                    errors_left.append(error)
                elif name == 'center':
                    errors_center.append(error)
                else:
                    errors_right.append(error)
                
                all_preds[name].append(pred_depths[name])
                all_gts[name].append(gt_depths[name])
            
            # ========== ç¹ªè£½ç•«å¸ƒ ==========
            canvas = np.zeros((frame_height, frame_width, 3), dtype=np.uint8)
            canvas[:] = (30, 30, 30)
            
            # ========== å·¦ä¸Š: RGB åœ– + å€åŸŸæ¨™è¨˜ ==========
            img_with_regions = img_resized.copy()
            h_vis, w_vis = img_with_regions.shape[:2]
            y1, y2 = h_vis//4, 3*h_vis//4
            
            colors = {'left': (100, 100, 255), 'center': (100, 255, 100), 'right': (255, 100, 100)}
            boxes = {
                'left': (0, y1, w_vis//3, y2),
                'center': (w_vis//3, y1, 2*w_vis//3, y2),
                'right': (2*w_vis//3, y1, w_vis, y2),
            }
            
            for name, (x1, y1_b, x2, y2_b) in boxes.items():
                cv2.rectangle(img_with_regions, (x1, y1_b), (x2, y2_b), colors[name], 2)
            
            canvas[20:380, 20:660] = img_with_regions
            cv2.putText(canvas, 'RGB Image + Region Crops', (25, 400),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200, 200, 200), 1)
            
            # ========== å³ä¸Š: æ·±åº¦åœ– ==========
            depth_vis = cv2.applyColorMap(
                (np.clip(depth_resized / 5.0, 0, 1) * 255).astype(np.uint8),
                cv2.COLORMAP_JET
            )
            canvas[20:380, 680:1260] = cv2.resize(depth_vis, (580, 360))
            cv2.putText(canvas, 'GT Depth Map (colormap)', (685, 400),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200, 200, 200), 1)
            
            # ========== ä¸‹åŠéƒ¨: æ·±åº¦é æ¸¬æ¯”è¼ƒ ==========
            panel_y = 440
            
            cv2.putText(canvas, 'Depth Regression: Prediction vs Ground Truth', (20, panel_y),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)
            cv2.putText(canvas, f'Frame: {i+1}/{len(frame_files)}', (550, panel_y),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (200, 200, 200), 1)
            
            # ä¸‰å€‹å€åŸŸçš„æ¯”è¼ƒæŸ±ç‹€åœ–
            bar_start_x = 50
            bar_width = 80
            bar_gap = 120
            bar_max_height = 150
            bar_y_base = 650
            
            region_names = ['Left', 'Center', 'Right']
            region_keys = ['left', 'center', 'right']
            
            for j, (name, key) in enumerate(zip(region_names, region_keys)):
                x_center = bar_start_x + j * (bar_width + bar_gap) + bar_width // 2
                
                # GT bar (è—è‰²)
                gt_h = int((gt_depths[key] / 5.0) * bar_max_height)
                gt_h = min(gt_h, bar_max_height)
                cv2.rectangle(canvas, 
                             (x_center - 35, bar_y_base - gt_h),
                             (x_center - 5, bar_y_base),
                             (255, 150, 50), -1)
                
                # Pred bar (ç¶ è‰²)
                pred_h = int((pred_depths[key] / 5.0) * bar_max_height)
                pred_h = min(pred_h, bar_max_height)
                cv2.rectangle(canvas,
                             (x_center + 5, bar_y_base - pred_h),
                             (x_center + 35, bar_y_base),
                             (50, 255, 50), -1)
                
                # å€åŸŸåç¨±
                cv2.putText(canvas, name, (x_center - 25, bar_y_base + 25),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, colors[key], 1)
                
                # æ•¸å€¼æ¨™ç±¤
                cv2.putText(canvas, f'GT:{gt_depths[key]:.2f}m', (x_center - 45, panel_y + 50),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.45, (255, 150, 50), 1)
                cv2.putText(canvas, f'Pred:{pred_depths[key]:.2f}m', (x_center - 45, panel_y + 75),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.45, (50, 255, 50), 1)
                
                # èª¤å·®
                error = abs(pred_depths[key] - gt_depths[key])
                err_color = (0, 255, 0) if error < 0.5 else (0, 165, 255) if error < 1.0 else (0, 0, 255)
                cv2.putText(canvas, f'Err:{error:.2f}m', (x_center - 40, panel_y + 100),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.45, err_color, 1)
            
            # åœ–ä¾‹
            legend_x = 450
            legend_y = panel_y + 50
            cv2.rectangle(canvas, (legend_x, legend_y), (legend_x + 20, legend_y + 15), (255, 150, 50), -1)
            cv2.putText(canvas, 'Ground Truth', (legend_x + 30, legend_y + 12),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 150, 50), 1)
            cv2.rectangle(canvas, (legend_x, legend_y + 25), (legend_x + 20, legend_y + 40), (50, 255, 50), -1)
            cv2.putText(canvas, 'Prediction', (legend_x + 30, legend_y + 37),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (50, 255, 50), 1)
            
            # ========== å³ä¸‹: èª¤å·®æ›²ç·š ==========
            graph_x = 650
            graph_y = panel_y + 40
            graph_w = 350
            graph_h = 120
            
            cv2.rectangle(canvas, (graph_x, graph_y), (graph_x + graph_w, graph_y + graph_h),
                         (50, 50, 50), -1)
            
            # ç¹ªè£½èª¤å·®æ›²ç·š
            if len(errors_center) > 1:
                max_err = 2.0  # æœ€å¤§èª¤å·® 2m
                
                # 1m åƒè€ƒç·š
                ref_y = graph_y + graph_h - int(1.0 / max_err * graph_h)
                cv2.line(canvas, (graph_x, ref_y), (graph_x + graph_w, ref_y), (100, 100, 100), 1)
                cv2.putText(canvas, '1m', (graph_x - 25, ref_y + 5),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.35, (100, 100, 100), 1)
                
                # ç¹ªè£½ä¸‰æ¢æ›²ç·š
                for errors, color, name in [
                    (errors_left, (100, 100, 255), 'L'),
                    (errors_center, (100, 255, 100), 'C'),
                    (errors_right, (255, 100, 100), 'R'),
                ]:
                    points = []
                    for k, err in enumerate(errors[-50:]):  # æœ€è¿‘ 50 å¹€
                        x = graph_x + int(k * graph_w / 50)
                        y = graph_y + graph_h - int(min(err, max_err) / max_err * graph_h)
                        points.append((x, y))
                    
                    if len(points) > 1:
                        for k in range(len(points) - 1):
                            cv2.line(canvas, points[k], points[k+1], color, 1)
                
                cv2.putText(canvas, 'Prediction Error (m) - L/C/R', (graph_x, graph_y - 5),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.4, (200, 200, 200), 1)
            
            # ========== çµ±è¨ˆæ•¸æ“š ==========
            stats_x = 650
            stats_y = graph_y + graph_h + 30
            
            avg_error = (np.mean(errors_left) + np.mean(errors_center) + np.mean(errors_right)) / 3 if errors_center else 0
            cv2.putText(canvas, f'Mean Abs Error: {avg_error:.3f}m', (stats_x, stats_y),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)
            
            # è©•åˆ†
            if avg_error < 0.3:
                grade, grade_color = 'Excellent', (0, 255, 0)
            elif avg_error < 0.5:
                grade, grade_color = 'Good', (0, 255, 255)
            elif avg_error < 1.0:
                grade, grade_color = 'Fair', (0, 165, 255)
            else:
                grade, grade_color = 'Poor', (0, 0, 255)
            
            cv2.putText(canvas, f'Grade: {grade}', (stats_x + 250, stats_y),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, grade_color, 2)
            
            # æ¨™é¡Œ
            cv2.putText(canvas, f'TempoVLM: Depth Regression Demo', (20, frame_height - 10),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (100, 100, 100), 1)
            
            out.write(canvas)
        
        out.release()
        
        # è¼¸å‡ºçµ±è¨ˆ
        if errors_center:
            avg_err_all = (np.mean(errors_left) + np.mean(errors_center) + np.mean(errors_right)) / 3
            print(f"  ğŸ“Š å¹³å‡æ·±åº¦é æ¸¬èª¤å·®: {avg_err_all:.3f}m")
            print(f"      Left: {np.mean(errors_left):.3f}m, Center: {np.mean(errors_center):.3f}m, Right: {np.mean(errors_right):.3f}m")
        
        print(f"âœ… å½±ç‰‡å·²ä¿å­˜: {output_path}")
    
    # ========== 3. é‹å‹•æ„ŸçŸ¥è¦–è¦ºåŒ– ==========
    
    def visualize_trajectory(self, scene_dir, output_path, max_frames=100):
        """
        ç”Ÿæˆå³æ™‚è»Œè·¡å½±ç‰‡
        - ä¸»ç•«é¢: åŸå§‹å½±åƒ
        - å³ä¸‹è§’: ä¿¯è¦–è»Œè·¡åœ–
        """
        print("\nğŸ¬ ç”Ÿæˆè»Œè·¡å½±ç‰‡...")
        
        color_dir = scene_dir / 'color'
        pose_dir = scene_dir / 'pose'
        
        frame_files = sorted(color_dir.glob('*.jpg'))[:max_frames]
        
        if len(frame_files) < 10:
            print("âŒ å¹€æ•¸ä¸è¶³")
            return
        
        # è¼‰å…¥ GT è»Œè·¡
        gt_positions = []
        for frame_file in frame_files:
            pose_file = pose_dir / (frame_file.stem + '.txt')
            if pose_file.exists():
                try:
                    pose = np.loadtxt(pose_file).reshape(4, 4)
                    gt_positions.append(pose[:3, 3])
                except:
                    gt_positions.append(gt_positions[-1] if gt_positions else np.zeros(3))
            else:
                gt_positions.append(gt_positions[-1] if gt_positions else np.zeros(3))
        
        gt_positions = np.array(gt_positions)
        
        # é æ¸¬è»Œè·¡
        print("  é æ¸¬é‹å‹•...")
        pred_positions = [np.zeros(3)]
        prev_feat = None
        
        for i, frame_file in enumerate(tqdm(frame_files, desc="  æå–ç‰¹å¾µ")):
            img = Image.open(frame_file).convert('RGB')
            feat = self.extract_features(img)
            
            if prev_feat is not None:
                with torch.no_grad():
                    outputs = self.unified_model(feat, prev_feat, tasks=['motion'])
                    pred_motion = outputs['motion'].cpu().numpy()[0]
                    pred_positions.append(pred_positions[-1] + pred_motion[:3])
            
            prev_feat = feat
        
        pred_positions = np.array(pred_positions)
        
        # å°é½Šé æ¸¬è»Œè·¡
        pred_centered = pred_positions - pred_positions.mean(axis=0)
        gt_centered = gt_positions - gt_positions.mean(axis=0)
        
        pred_scale = np.linalg.norm(pred_centered)
        gt_scale = np.linalg.norm(gt_centered)
        
        if pred_scale > 1e-6:
            pred_scaled = pred_centered * (gt_scale / pred_scale)
        else:
            pred_scaled = pred_centered
        
        # ç”Ÿæˆå½±ç‰‡
        frame_width = 1280
        frame_height = 720
        fps = 10
        
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(str(output_path), fourcc, fps, (frame_width, frame_height))
        
        for i, frame_file in enumerate(tqdm(frame_files, desc="  å¯«å…¥å¹€")):
            img = cv2.imread(str(frame_file))
            img = cv2.resize(img, (800, 600))
            
            # å‰µå»ºç•«å¸ƒ
            canvas = np.zeros((frame_height, frame_width, 3), dtype=np.uint8)
            canvas[:] = (30, 30, 30)
            
            # æ”¾ç½®ä¸»åœ–
            canvas[20:620, 20:820] = img
            
            # ç¹ªè£½ä¿¯è¦–åœ–
            traj_size = 400
            traj_x = 850
            traj_y = 20
            
            # èƒŒæ™¯
            cv2.rectangle(canvas, (traj_x, traj_y), (traj_x + traj_size, traj_y + traj_size),
                         (50, 50, 50), -1)
            
            # ç¶²æ ¼
            for j in range(5):
                offset = int(j * traj_size / 4)
                cv2.line(canvas, (traj_x + offset, traj_y), (traj_x + offset, traj_y + traj_size),
                        (70, 70, 70), 1)
                cv2.line(canvas, (traj_x, traj_y + offset), (traj_x + traj_size, traj_y + offset),
                        (70, 70, 70), 1)
            
            # è¨ˆç®—é¡¯ç¤ºç¯„åœ
            all_pos = np.vstack([gt_centered[:i+1], pred_scaled[:i+1]])
            if len(all_pos) > 0:
                x_range = max(abs(all_pos[:, 0].max()), abs(all_pos[:, 0].min()), 1.0)
                z_range = max(abs(all_pos[:, 2].max()), abs(all_pos[:, 2].min()), 1.0)
                scale = min(traj_size / (2.2 * x_range), traj_size / (2.2 * z_range))
            else:
                scale = 50
            
            center_x = traj_x + traj_size // 2
            center_y = traj_y + traj_size // 2
            
            # ç¹ªè£½ GT è»Œè·¡
            gt_points = []
            for j in range(i + 1):
                px = int(center_x + gt_centered[j, 0] * scale)
                py = int(center_y - gt_centered[j, 2] * scale)  # z è»¸å‘ä¸Š
                gt_points.append((px, py))
            
            if len(gt_points) > 1:
                for j in range(len(gt_points) - 1):
                    cv2.line(canvas, gt_points[j], gt_points[j+1], (100, 255, 100), 2)
            
            # ç¹ªè£½é æ¸¬è»Œè·¡
            if i < len(pred_scaled):
                pred_points = []
                for j in range(min(i + 1, len(pred_scaled))):
                    px = int(center_x + pred_scaled[j, 0] * scale)
                    py = int(center_y - pred_scaled[j, 2] * scale)
                    pred_points.append((px, py))
                
                if len(pred_points) > 1:
                    for j in range(len(pred_points) - 1):
                        cv2.line(canvas, pred_points[j], pred_points[j+1], (255, 100, 100), 2)
            
            # ç•¶å‰ä½ç½®æ¨™è¨˜
            if gt_points:
                cv2.circle(canvas, gt_points[-1], 8, (100, 255, 100), -1)
            if i < len(pred_scaled) and pred_points:
                cv2.circle(canvas, pred_points[-1], 8, (255, 100, 100), -1)
            
            # èµ·é»æ¨™è¨˜
            if gt_points:
                cv2.circle(canvas, gt_points[0], 5, (255, 255, 255), -1)
                cv2.putText(canvas, 'Start', (gt_points[0][0] + 10, gt_points[0][1]),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)
            
            # åœ–ä¾‹
            cv2.putText(canvas, 'Top-down Trajectory View', (traj_x, traj_y + traj_size + 30),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)
            
            legend_y = traj_y + traj_size + 60
            cv2.line(canvas, (traj_x, legend_y), (traj_x + 30, legend_y), (100, 255, 100), 2)
            cv2.putText(canvas, 'GT Trajectory', (traj_x + 40, legend_y + 5),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (100, 255, 100), 1)
            
            cv2.line(canvas, (traj_x, legend_y + 25), (traj_x + 30, legend_y + 25), (255, 100, 100), 2)
            cv2.putText(canvas, 'Predicted', (traj_x + 40, legend_y + 30),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 100, 100), 1)
            
            # èª¤å·®çµ±è¨ˆ
            if i > 0 and i < len(pred_scaled):
                error = np.sqrt(((gt_centered[:i+1] - pred_scaled[:i+1]) ** 2).sum(axis=1).mean())
                cv2.putText(canvas, f'ATE: {error:.3f}m', (traj_x, legend_y + 70),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)
            
            # æ¨™é¡Œ
            cv2.putText(canvas, f'TempoVLM: Motion Prediction Demo - Frame {i+1}/{len(frame_files)}',
                       (20, frame_height - 20),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (150, 150, 150), 1)
            
            out.write(canvas)
        
        out.release()
        print(f"âœ… å½±ç‰‡å·²ä¿å­˜: {output_path}")
    
    # ========== ä¸»å‡½æ•¸ ==========
    
    def run_all_visualizations(self, data_root, output_dir, split='test', max_scenes=5):
        """åŸ·è¡Œæ‰€æœ‰è¦–è¦ºåŒ–"""
        data_root = Path(data_root)
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # æ ¹æ“š split é¸æ“‡è³‡æ–™å¤¾
        if split == 'test':
            search_dirs = ['scannet_frames_test']
            print("\nğŸ§ª ä½¿ç”¨æ¸¬è©¦é›† (scannet_frames_test)")
        elif split == 'train':
            search_dirs = ['scannet_frames_25k']
            print("\nğŸ“š ä½¿ç”¨è¨“ç·´é›† (scannet_frames_25k)")
        else:
            search_dirs = ['scannet_frames_test', 'scannet_frames_25k', '']
        
        # æ‰¾å ´æ™¯
        scene_dirs = []
        for subdir in search_dirs:
            subpath = data_root / subdir if subdir else data_root
            if subpath.exists():
                scenes = [d for d in subpath.iterdir() if d.is_dir() and d.name.startswith('scene')]
                scene_dirs.extend(scenes)
        
        if not scene_dirs:
            print("âŒ æ‰¾ä¸åˆ°å ´æ™¯")
            return
        
        # é¸æ“‡å¤šå€‹å ´æ™¯
        scene_dirs = sorted(scene_dirs)[:max_scenes]
        print(f"\nğŸ“Š å°‡è™•ç† {len(scene_dirs)} å€‹å ´æ™¯")
        
        # ç‚ºæ¯å€‹å ´æ™¯ç”Ÿæˆè¦–è¦ºåŒ–
        for idx, scene_dir in enumerate(scene_dirs):
            print(f"\n{'='*60}")
            print(f"ğŸ¬ å ´æ™¯ {idx+1}/{len(scene_dirs)}: {scene_dir.name}")
            print(f"{'='*60}")
            
            # å‰µå»ºå ´æ™¯å°ˆå±¬è¼¸å‡ºç›®éŒ„
            scene_output_dir = output_dir / scene_dir.name
            scene_output_dir.mkdir(parents=True, exist_ok=True)
            
            try:
                # 1. æ™‚åºä¸€è‡´æ€§
                self.visualize_temporal_consistency(
                    scene_dir, 
                    scene_output_dir / 'temporal_consistency.mp4'
                )
                
                # 2. æ·±åº¦æ’åº (depth_order)
                self.visualize_depth_radar(
                    scene_dir,
                    scene_output_dir / 'depth_ordering.mp4'
                )
                
                # 3. æ·±åº¦å›æ­¸ (depth_regression) - éœ€è¦æœ‰è¨“ç·´éçš„æ¨¡å‹
                if hasattr(self.unified_model, 'depth_regression_head'):
                    self.visualize_depth_regression(
                        scene_dir,
                        scene_output_dir / 'depth_regression.mp4'
                    )
                
                # 4. è»Œè·¡
                self.visualize_trajectory(
                    scene_dir,
                    scene_output_dir / 'trajectory.mp4'
                )
                
                print(f"âœ… {scene_dir.name} å®Œæˆ")
                
            except Exception as e:
                print(f"âŒ {scene_dir.name} å¤±æ•—: {e}")
                continue
        
        # ç”Ÿæˆç¸½çµ
        self._generate_summary(output_dir, scene_dirs)
        
        print(f"\nğŸ‰ æ‰€æœ‰è¦–è¦ºåŒ–å®Œæˆï¼è¼¸å‡ºç›®éŒ„: {output_dir}")
    
    def _generate_summary(self, output_dir, scene_dirs):
        """ç”Ÿæˆè¦–è¦ºåŒ–ç¸½çµ"""
        has_depth_regression = hasattr(self.unified_model, 'depth_regression_head')
        
        outputs = [
            'temporal_consistency.mp4',
            'depth_ordering.mp4',
            'trajectory.mp4'
        ]
        if has_depth_regression:
            outputs.insert(2, 'depth_regression.mp4')
        
        summary = {
            'total_scenes': len(scene_dirs),
            'scenes': [d.name for d in scene_dirs],
            'has_depth_regression': has_depth_regression,
            'outputs_per_scene': outputs
        }
        
        import json
        with open(output_dir / 'summary.json', 'w') as f:
            json.dump(summary, f, indent=2)
        
        # ç”Ÿæˆ README
        depth_reg_col = "| æ·±åº¦å›æ­¸ " if has_depth_regression else ""
        depth_reg_header = "|---------|" if has_depth_regression else ""
        
        readme = f"""# TempoVLM Visualization Results

## ç¸½å…± {len(scene_dirs)} å€‹å ´æ™¯

| å ´æ™¯ | æ™‚åºä¸€è‡´æ€§ | æ·±åº¦æ’åº {depth_reg_col}| è»Œè·¡é æ¸¬ |
|------|-----------|---------|{depth_reg_header}---------|
"""
        for scene_dir in scene_dirs:
            scene_name = scene_dir.name
            depth_reg_check = "| âœ… " if has_depth_regression else ""
            readme += f"| {scene_name} | âœ… | âœ… {depth_reg_check}| âœ… |\n"
        
        readme += f"""
## æ¯å€‹å ´æ™¯åŒ…å«:

1. **temporal_consistency.mp4** - æ™‚åºä¸€è‡´æ€§å°æ¯”å½±ç‰‡
   - å·¦å´: åŸå§‹å½±åƒ
   - å³å´: Base vs Unified ç›¸ä¼¼åº¦æ›²ç·š

2. **depth_ordering.mp4** - æ·±åº¦æ’åºæ¸¬è©¦
   - é æ¸¬ä¸‰å€‹å€åŸŸ (å·¦/ä¸­/å³) å“ªå€‹æ›´è¿‘
   - èˆ‡ Ground Truth æ·±åº¦åœ–æ¯”è¼ƒ
"""
        
        if has_depth_regression:
            readme += """
3. **depth_regression.mp4** - æ·±åº¦æ•¸å€¼é æ¸¬ â­ NEW
   - é æ¸¬æ¯å€‹å€åŸŸçš„çµ•å°æ·±åº¦å€¼ (ç±³)
   - æŸ±ç‹€åœ–æ¯”è¼ƒ: GT vs Prediction
   - å³æ™‚é¡¯ç¤ºé æ¸¬èª¤å·®æ›²ç·š
"""
            readme += """
4. **trajectory.mp4** - è»Œè·¡é æ¸¬
"""
        else:
            readme += """
3. **trajectory.mp4** - è»Œè·¡é æ¸¬
"""
        
        readme += """   - ä¿¯è¦–åœ–é¡¯ç¤º GT è»Œè·¡ vs é æ¸¬è»Œè·¡
   - ATE èª¤å·®å³æ™‚æ›´æ–°
"""
        
        with open(output_dir / 'README.md', 'w') as f:
            f.write(readme)


def main():
    parser = argparse.ArgumentParser(description='TempoVLM Visualization')
    parser.add_argument('--model_path', type=str, required=True)
    parser.add_argument('--data_root', type=str, required=True)
    parser.add_argument('--output_dir', type=str, default='./viz_demo')
    parser.add_argument('--split', type=str, default='test', choices=['train', 'test', 'all'],
                        help='ä½¿ç”¨å“ªå€‹è³‡æ–™é›†: train=scannet_frames_25k, test=scannet_frames_test')
    parser.add_argument('--max_scenes', type=int, default=5,
                        help='æœ€å¤šè™•ç†å¹¾å€‹å ´æ™¯')
    parser.add_argument('--device', type=str, default='cuda')
    
    args = parser.parse_args()
    
    visualizer = TempoVLMVisualizer(
        unified_model_path=args.model_path,
        device=args.device
    )
    
    visualizer.run_all_visualizations(
        data_root=args.data_root,
        output_dir=args.output_dir,
        split=args.split,
        max_scenes=args.max_scenes
    )


if __name__ == '__main__':
    main()
