#!/usr/bin/env python3
"""
occlusion_tester.py - é®æ“‹æ¸¬è©¦å™¨æ ¸å¿ƒé¡
======================================

åŒ…å« OcclusionTester é¡ï¼Œæä¾›:
- ç‰¹å¾µæå–
- å ´æ™¯æè¿°ç”Ÿæˆ
- Direct Feature Injection
- é®æ“‹æ‡‰ç”¨

ä½¿ç”¨æ–¹å¼:
    from occlusion_tester import OcclusionTester
    
    tester = OcclusionTester(unified_model_path='path/to/model.pt')
    
    # æå–ç‰¹å¾µ
    feat = tester.extract_features(image)
    
    # ç”Ÿæˆæè¿°
    desc = tester.generate_description(image, prompt="Describe.")
    
    # Direct Injection
    response = tester.generate_with_direct_injection(
        current_image=occluded_img,
        enhanced_feat=memory_feat,
        prompt="Describe the center.",
        injection_method='raw',
        injection_strength=0.4
    )
"""

import os
import torch
import torch.nn.functional as F
import numpy as np
from PIL import Image
import cv2

from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
from qwen_vl_utils import process_vision_info


class OcclusionTester:
    """é®æ“‹æ¸¬è©¦å™¨ - æ ¸å¿ƒé¡"""
    
    def __init__(self, unified_model_path=None, device='cuda'):
        self.device = device
        
        print("=" * 70)
        print("ğŸ§ª TempoVLM Occlusion Tester")
        print("=" * 70)
        
        # è¼‰å…¥ Qwen2-VL
        print("\nğŸ“¦ è¼‰å…¥ Qwen2-VL...")
        self.processor = AutoProcessor.from_pretrained(
            "Qwen/Qwen2-VL-2B-Instruct",
            trust_remote_code=True
        )
        self.base_model = Qwen2VLForConditionalGeneration.from_pretrained(
            "Qwen/Qwen2-VL-2B-Instruct",
            torch_dtype=torch.float16,
            device_map=device,
            trust_remote_code=True
        ).eval()

        self.unified_model = None
        
        self.temporal_buffer = []
        self.temporal_buffer_size = 8
        
        if unified_model_path and os.path.exists(unified_model_path):
            self._load_unified_model(unified_model_path)
        else:
            print(f"âš ï¸ æœªè¼‰å…¥ UnifiedTempoVLM (å°‡ä½¿ç”¨åŸå§‹ VLM ç‰¹å¾µ)")
            if unified_model_path:
                print(f"   è·¯å¾‘ä¸å­˜åœ¨: {unified_model_path}")
        
        # Feature Injection å…ƒä»¶
        self.feature_projector = None
        self._vision_hidden_size = None
        # æœ€æ–°ä¸€å¹€çš„ Adapter è¼¸å‡ºå“è³ª (GRU ç‰ˆæœ¬æ‰æœ‰)
        self.last_adapter_meta = None
        
        print("âœ… æ¨¡å‹è¼‰å…¥å®Œæˆï¼")
    
    def _load_unified_model(self, model_path):
        """è¼‰å…¥ Unified Modelï¼Œè‡ªå‹•ç›¸å®¹æ–°èˆŠç‰ˆæœ¬"""
        print(f"ğŸ“¦ è¼‰å…¥ UnifiedTempoVLM: {model_path}")
        
        from models_unified import UnifiedTempoVLM
        
        checkpoint = torch.load(model_path, map_location=self.device, weights_only=False)
        state_dict = checkpoint.get('model_state_dict', checkpoint)
        
        # æª¢æ¸¬ hidden_dim
        if 'shared_encoder.0.weight' in state_dict:
            hidden_dim = state_dict['shared_encoder.0.weight'].shape[0]
        else:
            hidden_dim = 768
        
        # æª¢æ¸¬æ˜¯å¦ç‚ºæ–°ç‰ˆ checkpointï¼ˆæœ‰ GRUï¼‰
        has_gru = any('temporal_gru' in k for k in state_dict.keys())
        
        # æª¢æ¸¬æ·±åº¦è¼¸å‡ºç¶­åº¦ï¼ˆæ–°ç‰ˆ=3, èˆŠç‰ˆ=1ï¼‰
        if 'depth_regression_head.2.bias' in state_dict:
            depth_dim = state_dict['depth_regression_head.2.bias'].shape[0]
        else:
            depth_dim = 3
        
        print(f"   Checkpoint é¡å‹: {'æ–°ç‰ˆ (GRU)' if has_gru else 'èˆŠç‰ˆ (ç„¡ GRU)'}")
        print(f"   hidden_dim: {hidden_dim}")
        print(f"   æ·±åº¦è¼¸å‡ºç¶­åº¦: {depth_dim}")
        
        # å»ºç«‹å°æ‡‰æ¶æ§‹çš„æ¨¡å‹
        self.unified_model = UnifiedTempoVLM(
            hidden_dim=hidden_dim,
            use_gru_memory=has_gru
        ).to(self.device)
        
        # è™•ç† size mismatch çš„å•é¡Œï¼ˆæ‰‹å‹•éæ¿¾ï¼‰
        model_state = self.unified_model.state_dict()
        filtered_state_dict = {}
        skipped_keys = []
        
        for k, v in state_dict.items():
            if k in model_state:
                if v.shape == model_state[k].shape:
                    filtered_state_dict[k] = v
                else:
                    skipped_keys.append(f"{k}: checkpoint {v.shape} vs model {model_state[k].shape}")
            else:
                # key ä¸å­˜åœ¨æ–¼æ¨¡å‹ä¸­ï¼Œè·³é
                pass
        
        if skipped_keys:
            print(f"   âš ï¸ è·³é size ä¸åŒ¹é…çš„æ¬Šé‡:")
            for sk in skipped_keys[:5]:
                print(f"      - {sk}")
        
        # è¼‰å…¥éæ¿¾å¾Œçš„ state_dict
        missing, unexpected = self.unified_model.load_state_dict(filtered_state_dict, strict=False)
        
        if missing:
            # éæ¿¾æ‰é æœŸå…§çš„ç¼ºå¤±ï¼ˆæ–°æ¨¡çµ„ï¼‰
            expected_missing = ['motion_scale', 'temporal_gru', 'memory_quality_gate', 
                               'memory_output_gate', 'motion_uncertainty_head', 
                               'velocity_smoothing', 'global_scale_head', 
                               'motion_quality_head', 'place_embedding',
                               'depth_regression_head']  # èˆŠç‰ˆæ·±åº¦ä¹Ÿç®—é æœŸå…§
            real_missing = [k for k in missing if not any(exp in k for exp in expected_missing)]
            if real_missing:
                print(f"   âš ï¸ ç¼ºå°‘éé æœŸçš„æ¬Šé‡: {real_missing[:5]}...")
        
        self.unified_model.eval()
        self.unified_model.float()  # ä½¿ç”¨ float32 é¿å…ç²¾åº¦å•é¡Œ
        
        # è¨˜éŒ„æ¨¡å‹é¡å‹
        self.use_gru = has_gru
        self.gru_hidden_state = None  # GRU éš±è—ç‹€æ…‹
        
        print(f"âœ… UnifiedTempoVLM è¼‰å…¥å®Œæˆ (GRUè¨˜æ†¶: {'å•Ÿç”¨' if has_gru else 'åœç”¨'})")
    
    def extract_features(self, image, use_adapter=True):
        """
        æå–ç‰¹å¾µ (Qwen2-VL hidden states)
        
        Args:
            image: è¼¸å…¥åœ–åƒ
            use_adapter: æ˜¯å¦ä½¿ç”¨ Adapter å¢å¼·ç‰¹å¾µï¼ˆé è¨­ Trueï¼‰
        
        Returns:
            features: å¦‚æœ use_adapter=True ä¸”æœ‰ Adapterï¼Œè¿”å›å¢å¼·å¾Œçš„ç‰¹å¾µ
                     å¦å‰‡è¿”å›åŸå§‹ VLM ç‰¹å¾µ
        """
        if isinstance(image, np.ndarray):
            image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
        
        messages = [{
            "role": "user",
            "content": [
                {"type": "image", "image": image},
                {"type": "text", "text": "Describe."}
            ]
        }]
        
        text = self.processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        image_inputs, video_inputs = process_vision_info(messages)
        inputs = self.processor(
            text=[text],
            images=image_inputs,
            videos=video_inputs,
            padding=True,
            return_tensors="pt"
        ).to(self.device)
        
        with torch.no_grad():
            outputs = self.base_model(
                **inputs,
                output_hidden_states=True,
                return_dict=True
            )
            raw_features = outputs.hidden_states[-1].mean(dim=1)
        
        # å¦‚æœæœ‰ Adapter ä¸”å•Ÿç”¨ï¼Œä½¿ç”¨ Adapter å¢å¼·ç‰¹å¾µ
        # é‡è¨­ä¸Šä¸€å¹€çš„ Adapter å“è³ªè³‡è¨Š
        self.last_adapter_meta = None
        if use_adapter and self.unified_model is not None:
            enhanced_features = self._enhance_with_adapter(raw_features)
            return enhanced_features
        
        return raw_features
    
    def _enhance_with_adapter(self, current_feat):
        """
        ä½¿ç”¨ UnifiedTempoVLM Adapter å¢å¼·ç‰¹å¾µ
        
        æ•´åˆæ™‚åºè³‡è¨Šï¼Œè®“ç‰¹å¾µåŒ…å«éå»å¹€çš„ä¸Šä¸‹æ–‡
        æ”¯æ´æ–°ç‰ˆ (GRU) å’ŒèˆŠç‰ˆæ¨¡å‹
        
        Args:
            current_feat: ç•¶å‰å¹€çš„åŸå§‹ç‰¹å¾µ [1, feat_dim]
        
        Returns:
            enhanced_feat: æ™‚åºå¢å¼·å¾Œçš„ç‰¹å¾µ [1, feat_dim]
        """
        # ç¶­è­·ç·©è¡å€å¤§å°
        if len(self.temporal_buffer) > self.temporal_buffer_size:
            self.temporal_buffer.pop(0)
        
        # å¦‚æœç·©è¡å€å¹€æ•¸ä¸è¶³ï¼Œç›´æ¥è¿”å›åŸå§‹ç‰¹å¾µ
        if len(self.temporal_buffer) < 1:
            # ç¬¬ä¸€å¹€ï¼Œæ²’æœ‰å‰ä¸€å¹€å¯ç”¨
            self.temporal_buffer.append(current_feat.clone())
            return current_feat
        
        try:
            with torch.no_grad():
                # UnifiedTempoVLM.forward() éœ€è¦ curr_feat å’Œ prev_feat
                # curr_feat: [B, feat_dim], prev_feat: [B, feat_dim]
                prev_feat = self.temporal_buffer[-1]  # æœ€å¾Œä¸€å¹€ä½œç‚º prev
                
                # ç¢ºä¿ç¶­åº¦æ­£ç¢º [1, feat_dim]
                if current_feat.dim() == 1:
                    current_feat = current_feat.unsqueeze(0)
                if prev_feat.dim() == 1:
                    prev_feat = prev_feat.unsqueeze(0)
                
                # ç¢ºä¿ä½¿ç”¨ float32
                current_feat = current_feat.float()
                prev_feat = prev_feat.float()
                
                adapter_meta = {}
                if hasattr(self, 'use_gru') and self.use_gru:
                    outputs, self.gru_hidden_state = self.unified_model(
                        curr_feat=current_feat,
                        prev_feat=prev_feat,
                        hidden_state=self.gru_hidden_state,
                        tasks=['temporal']
                    )
                else:
                    result = self.unified_model(
                        curr_feat=current_feat,
                        prev_feat=prev_feat,
                        tasks=['temporal']
                    )
                    if isinstance(result, tuple):
                        outputs, _ = result
                    else:
                        outputs = result
                
                if isinstance(outputs, dict) and 'temporal' in outputs and outputs['temporal'] is not None:
                    enhanced_current = outputs['temporal']  # [1, feat_dim]
                    # æ•æ‰æ–°æ¶æ§‹çš„å“è³ªé–€æ§è¨Šæ¯ï¼ˆå¦‚æœæœ‰ï¼‰
                    if 'memory_quality' in outputs and outputs['memory_quality'] is not None:
                        adapter_meta['memory_quality'] = float(outputs['memory_quality'])
                    if 'temporal_gate' in outputs and outputs['temporal_gate'] is not None:
                        adapter_meta['temporal_gate'] = float(outputs['temporal_gate'])
                else:
                    # æ²’æœ‰ temporal è¼¸å‡ºï¼Œè¿”å›åŸå§‹ç‰¹å¾µ
                    enhanced_current = current_feat
                
                # æ›´æ–°ç·©è¡å€ï¼ˆç”¨åŸå§‹ç‰¹å¾µï¼Œä¸æ˜¯å¢å¼·å¾Œçš„ï¼‰
                self.temporal_buffer.append(current_feat.clone())
                self.last_adapter_meta = adapter_meta if adapter_meta else None
                
                return enhanced_current
                
        except Exception as e:
            print(f"âš ï¸ Adapter å¢å¼·å¤±æ•—: {e}")
            # ä»ç„¶æ›´æ–°ç·©è¡å€
            self.temporal_buffer.append(current_feat.clone())
            self.last_adapter_meta = None
            return current_feat
    
    def extract_features_raw(self, image):
        """
        æå–åŸå§‹ç‰¹å¾µï¼ˆä¸ç¶“é Adapterï¼‰
        
        ç”¨æ–¼éœ€è¦åŸå§‹ç‰¹å¾µçš„å ´æ™¯ï¼Œå¦‚ç•°å¸¸æª¢æ¸¬
        """
        return self.extract_features(image, use_adapter=False)
    
    def extract_edge_features(self, image, edge_ratio=0.25):
        """
        æå–åœ–åƒé‚Šç·£å€åŸŸçš„ç‰¹å¾µ
        
        ç”¨æ–¼å ´æ™¯è®ŠåŒ–æª¢æ¸¬ - é‚Šç·£é€šå¸¸ä¸å—ä¸­å¿ƒé®æ“‹å½±éŸ¿
        
        Args:
            image: è¼¸å…¥åœ–åƒ
            edge_ratio: é‚Šç·£å€åŸŸä½”æ¯”ï¼ˆå¾æ¯é‚Šå–é€™å€‹æ¯”ä¾‹ï¼‰
        
        Returns:
            edge_feat: é‚Šç·£å€åŸŸçš„ç‰¹å¾µ
        """
        if isinstance(image, np.ndarray):
            img_array = image
        else:
            img_array = np.array(image)
        
        h, w = img_array.shape[:2]
        edge_h = int(h * edge_ratio)
        edge_w = int(w * edge_ratio)
        
        # æå–å››å€‹é‚Šç·£å€åŸŸ
        top = img_array[:edge_h, :, :]
        bottom = img_array[-edge_h:, :, :]
        left = img_array[edge_h:-edge_h, :edge_w, :]
        right = img_array[edge_h:-edge_h, -edge_w:, :]
        
        # çµ„åˆé‚Šç·£å€åŸŸæˆä¸€å¼µåœ–
        # ä½¿ç”¨ä¸Šé‚Šç·£å’Œä¸‹é‚Šç·£çš„å¹³å‡ç‰¹å¾µï¼ˆæœ€ç©©å®šï¼‰
        edge_combined = np.vstack([top, bottom])
        
        edge_img = Image.fromarray(edge_combined)
        
        # æå–ç‰¹å¾µï¼ˆä¸ä½¿ç”¨ Adapterï¼Œé¿å…æ™‚åºå¹²æ“¾ï¼‰
        return self.extract_features(edge_img, use_adapter=False)
    
    def clear_temporal_buffer(self):
        """æ¸…ç©ºæ™‚åºç·©è¡å€å’Œ GRU éš±è—ç‹€æ…‹ï¼ˆå ´æ™¯åˆ‡æ›æ™‚ä½¿ç”¨ï¼‰"""
        self.temporal_buffer = []
        if hasattr(self, 'gru_hidden_state'):
            self.gru_hidden_state = None
        self.last_adapter_meta = None
    
    def compute_similarity(self, feat1, feat2):
        """è¨ˆç®—é¤˜å¼¦ç›¸ä¼¼åº¦"""
        return F.cosine_similarity(
            feat1.flatten().unsqueeze(0),
            feat2.flatten().unsqueeze(0)
        ).item()
    
    def generate_description(self, image, prompt=None):
        """ç”Ÿæˆå ´æ™¯æè¿°"""
        if isinstance(image, np.ndarray):
            image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
        
        if prompt is None:
            prompt = "Describe what you see in this image in one sentence."
        
        messages = [{
            "role": "user",
            "content": [
                {"type": "image", "image": image},
                {"type": "text", "text": prompt}
            ]
        }]
        
        text = self.processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        image_inputs, video_inputs = process_vision_info(messages)
        inputs = self.processor(
            text=[text],
            images=image_inputs,
            videos=video_inputs,
            padding=True,
            return_tensors="pt"
        ).to(self.device)
        
        with torch.no_grad():
            generated = self.base_model.generate(
                **inputs,
                max_new_tokens=100,
                do_sample=False
            )
        
        response = self.processor.decode(generated[0], skip_special_tokens=True)
        if "assistant" in response.lower():
            response = response.split("assistant")[-1].strip()
        return response
    
    def _get_vision_hidden_size(self):
        """ç²å–è¦–è¦ºç‰¹å¾µç¶­åº¦"""
        if self._vision_hidden_size is None:
            try:
                self._vision_hidden_size = self.base_model.config.vision_config.hidden_size
            except:
                self._vision_hidden_size = 1536
        return self._vision_hidden_size
    
    def _init_feature_projector(self, source_dim, target_dim):
        """åˆå§‹åŒ–ç‰¹å¾µæŠ•å½±å±¤"""
        if self.feature_projector is None or \
           self.feature_projector.in_features != source_dim or \
           self.feature_projector.out_features != target_dim:
            
            self.feature_projector = torch.nn.Linear(source_dim, target_dim)
            with torch.no_grad():
                if source_dim == target_dim:
                    torch.nn.init.eye_(self.feature_projector.weight)
                else:
                    torch.nn.init.xavier_uniform_(self.feature_projector.weight)
                torch.nn.init.zeros_(self.feature_projector.bias)
            
            self.feature_projector = self.feature_projector.to(self.device).half()
    
    def generate_with_direct_injection(
        self, 
        current_image, 
        enhanced_feat,
        prompt=None,
        injection_method='raw',
        injection_strength=0.4
    ):
        """
        ğŸ§  Direct Feature Injection
        
        å°‡è¨˜æ†¶ç‰¹å¾µæ³¨å…¥åˆ°è¦–è¦ºç·¨ç¢¼å™¨è¼¸å‡ºä¸­
        
        Args:
            current_image: ç•¶å‰å¹€ï¼ˆå¯èƒ½è¢«é®æ“‹ï¼‰
            enhanced_feat: è¦æ³¨å…¥çš„è¨˜æ†¶ç‰¹å¾µ
            prompt: æå•
            injection_method: 'raw', 'full', 'strong', 'adaptive'
            injection_strength: æ³¨å…¥å¼·åº¦ (0-1)
        
        Returns:
            str: ç”Ÿæˆçš„å›ç­”
        """
        if isinstance(current_image, np.ndarray):
            current_image = Image.fromarray(cv2.cvtColor(current_image, cv2.COLOR_BGR2RGB))
        
        if prompt is None:
            prompt = "What objects are in the center of this image?"
        
        messages = [{
            "role": "user",
            "content": [
                {"type": "image", "image": current_image},
                {"type": "text", "text": prompt}
            ]
        }]
        
        text = self.processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        image_inputs, video_inputs = process_vision_info(messages)
        inputs = self.processor(
            text=[text],
            images=image_inputs,
            videos=video_inputs,
            padding=True,
            return_tensors="pt"
        ).to(self.device)
        
        enhanced_feat_copy = enhanced_feat.clone().detach()
        vision_hidden_size = self._get_vision_hidden_size()
        enhanced_dim = enhanced_feat_copy.shape[-1]
        
        if enhanced_dim != vision_hidden_size:
            self._init_feature_projector(enhanced_dim, vision_hidden_size)
        
        def create_injection_hook(method, strength):
            def injection_hook(module, input, output):
                nonlocal enhanced_feat_copy
                
                with torch.no_grad():
                    if enhanced_dim != vision_hidden_size:
                        projected = self.feature_projector(enhanced_feat_copy.float()).half()
                    else:
                        projected = enhanced_feat_copy
                    
                    if output.dim() == 2:
                        num_patches = output.shape[0]
                        projected_expanded = projected.squeeze(0).unsqueeze(0).expand(num_patches, -1)
                        batch = 1
                    elif output.dim() == 3:
                        batch, num_patches, _ = output.shape
                        projected_expanded = projected.unsqueeze(1).expand(batch, num_patches, -1)
                    else:
                        return output
                    
                    orig_mean = output.mean()
                    orig_std = output.std() + 1e-6
                    
                    # æ­£è¦åŒ–ï¼šé¿å…è¢«é®æ“‹åœ–çš„ä½æ–¹å·®æŠŠè¨˜æ†¶å£“æ‰ï¼Œæ”¹ç”¨è¨˜æ†¶çµ±è¨ˆç‚ºä¸»ã€åŸåœ–ç‚ºè¼”
                    proj_mean = projected_expanded.mean()
                    proj_std = projected_expanded.std() + 1e-6
                    blended_mean = 0.5 * proj_mean + 0.5 * orig_mean
                    blended_std = torch.max(0.7 * proj_std + 0.3 * orig_std, 0.5 * proj_std)
                    projected_normalized = (projected_expanded - proj_mean) / proj_std * blended_std + blended_mean
                    
                    # ç”Ÿæˆä¸­å¿ƒé®ç½©ï¼Œåªåœ¨ä¸­å¿ƒå€åŸŸæ³¨å…¥ï¼Œæ¸›å°‘å°å‘¨é‚Šä¸Šä¸‹æ–‡çš„å¹²æ“¾
                    if num_patches > 4:
                        side = int(num_patches ** 0.5)
                        if side * side == num_patches:
                            # ä»¥ Chebyshev è·é›¢è¡¡é‡ä¸­å¿ƒæ€§ï¼Œä¸­å¿ƒ 60% å…§æ³¨å…¥
                            idxs = torch.arange(num_patches, device=output.device).view(1, num_patches, 1)
                            rows = (idxs // side).float()
                            cols = (idxs % side).float()
                            center = (side - 1) / 2
                            dist = torch.maximum((rows - center).abs(), (cols - center).abs()) / (side / 2)
                            center_mask = (dist < 0.8).float()
                        else:
                            center_mask = torch.ones((1, num_patches, 1), device=output.device)
                    else:
                        center_mask = torch.ones((1, num_patches, 1), device=output.device)
                    # ä¸­å¿ƒåŠ æ¬Šæå‡ï¼Œè®“é®æ“‹å€åŸŸçš„æ³¨å…¥æ›´æ˜é¡¯
                    center_mask = torch.clamp(center_mask * 1.5, max=1.0)
                    
                    # ============================================================
                    # æ³¨å…¥æ–¹æ³•é¸æ“‡
                    # ============================================================
                    
                    if method == 'full':
                        # æ–¹æ³•1: å…¨åœ–æ³¨å…¥ (ä½†åƒ…ä¸­å¿ƒå€åŸŸ)
                        mix = strength * center_mask
                        if output.dim() == 3:
                            modified = output + mix * (projected_normalized - output)
                        else:
                            modified = output + mix.squeeze(0) * (projected_normalized - output)
                    
                    elif method == 'strong':
                        # æ–¹æ³•2: å¼·åŠ›ä¸­å¿ƒæ³¨å…¥ - ä¸­å¿ƒå€åŸŸé«˜å¼·åº¦ï¼Œé‚Šç·£ä½å¼·åº¦
                        modified = output.clone()
                        if output.dim() == 2:
                            num_patches = output.shape[0]
                            side = int(num_patches ** 0.5)
                            if side * side == num_patches:
                                for row in range(side):
                                    for col in range(side):
                                        idx = row * side + col
                                        # è¨ˆç®—åˆ°ä¸­å¿ƒçš„è·é›¢
                                        dist_to_center = max(abs(row - side/2), abs(col - side/2)) / (side/2)
                                        # ä¸­å¿ƒå¼·åº¦é«˜ï¼Œé‚Šç·£å¼·åº¦ä½
                                        local_strength = strength * (1 - dist_to_center * 0.5)
                                        local_strength *= center_mask.view(-1)[idx].item()
                                        modified[idx] = (1 - local_strength) * output[idx] + local_strength * projected_normalized[idx]
                        elif output.dim() == 3:
                            batch_size, num_patches, _ = output.shape
                            side = int(num_patches ** 0.5)
                            if side * side == num_patches:
                                for row in range(side):
                                    for col in range(side):
                                        idx = row * side + col
                                        dist_to_center = max(abs(row - side/2), abs(col - side/2)) / (side/2)
                                        local_strength = strength * (1 - dist_to_center * 0.5)
                                        local_strength = local_strength * center_mask[:, idx, :].squeeze(-1)
                                        modified[:, idx] = (1 - local_strength) * output[:, idx] + local_strength * projected_normalized[:, idx]
                            else:
                                mix = strength * center_mask
                                modified = output + mix * (projected_normalized - output)
                    
                    elif method == 'adaptive':
                        # æ–¹æ³•3: è‡ªé©æ‡‰æ³¨å…¥ - æ ¹æ“šç‰¹å¾µå·®ç•°æ±ºå®šæ³¨å…¥å¼·åº¦
                        # å·®ç•°å¤§çš„ patch æ³¨å…¥æ›´å¤š
                        if output.dim() == 3:
                            # [batch, num_patches, hidden]
                            diff = torch.abs(output - projected_normalized).mean(dim=-1, keepdim=True)
                            diff_normalized = diff / (diff.max() + 1e-6)
                            # å·®ç•°è¶Šå¤§ï¼Œæ³¨å…¥è¶Šå¼·ï¼ˆå› ç‚ºå¯èƒ½æ˜¯é®æ“‹å€åŸŸï¼‰
                            adaptive_strength = strength * (0.5 + 0.5 * diff_normalized) * center_mask
                            modified = (1 - adaptive_strength) * output + adaptive_strength * projected_normalized
                        else:
                            diff = torch.abs(output - projected_normalized).mean(dim=-1, keepdim=True)
                            diff_normalized = diff / (diff.max() + 1e-6)
                            adaptive_strength = strength * (0.5 + 0.5 * diff_normalized) * center_mask.squeeze(0)
                            modified = (1 - adaptive_strength) * output + adaptive_strength * projected_normalized
                    
                    else:  # 'raw' æˆ–å…¶ä»–
                        # æ–¹æ³•4: åŸå§‹ä¸­å¿ƒæ³¨å…¥ï¼ˆä¿å®ˆï¼‰
                        mix = strength * center_mask
                        if output.dim() == 3:
                            modified = output + mix * (projected_normalized - output)
                        else:
                            modified = output + mix.squeeze(0) * (projected_normalized - output)
                    
                    # é™åˆ¶æ•¸å€¼ç¯„åœ
                    modified = torch.clamp(modified, orig_mean - 4*orig_std, orig_mean + 4*orig_std)
                    return modified
            
            return injection_hook
        
        hook_handle = self.base_model.visual.register_forward_hook(
            create_injection_hook(injection_method, injection_strength)
        )
        
        try:
            with torch.no_grad():
                generated = self.base_model.generate(
                    **inputs,
                    max_new_tokens=100,
                    do_sample=False
                )
        finally:
            hook_handle.remove()
        
        # è§£ç¢¼ç”Ÿæˆçš„æ–‡æœ¬
        full_response = self.processor.decode(generated[0], skip_special_tokens=True)
        
        # æå– assistant å›ç­”éƒ¨åˆ†
        response = full_response
        
        # å˜—è©¦å¤šç¨®åˆ†éš”æ–¹å¼
        separators = ['assistant\n', 'assistant:', 'Assistant:', 'ASSISTANT:', '<|assistant|>']
        for sep in separators:
            if sep.lower() in response.lower():
                idx = response.lower().find(sep.lower())
                response = response[idx + len(sep):].strip()
                break
        
        # å¦‚æœå›æ‡‰ä»ç„¶åŒ…å« promptï¼Œå˜—è©¦ç§»é™¤
        if prompt and prompt in response:
            response = response.split(prompt)[-1].strip()
        
        # å¦‚æœå›æ‡‰ç‚ºç©ºï¼Œè¿”å›åŸå§‹è¼¸å‡ºï¼ˆç”¨æ–¼ debugï¼‰
        if not response.strip():
            # å¯èƒ½æ˜¯è¼¸å…¥å’Œè¼¸å‡ºæ··åœ¨ä¸€èµ·ï¼Œå–æœ€å¾Œä¸€æ®µ
            lines = full_response.strip().split('\n')
            if lines:
                response = lines[-1].strip()
        
        return response
    
    def apply_occlusion(self, image, occ_type='box', ratio=0.3):

        if isinstance(image, Image.Image):
            img_array = np.array(image)
        else:
            img_array = image.copy()
        
        h, w = img_array.shape[:2]
        cx, cy = w // 2, h // 2
        
        occ_w = int(w * ratio)
        occ_h = int(h * ratio)
        x1, y1 = cx - occ_w // 2, cy - occ_h // 2
        x2, y2 = cx + occ_w // 2, cy + occ_h // 2
        
        if occ_type == 'box':
            img_array[y1:y2, x1:x2] = 0
        elif occ_type == 'noise':
            noise = np.random.randint(0, 255, (y2-y1, x2-x1, 3), dtype=np.uint8)
            img_array[y1:y2, x1:x2] = noise
        elif occ_type == 'blur':
            roi = img_array[y1:y2, x1:x2]
            blurred = cv2.GaussianBlur(roi, (99, 99), 0)
            img_array[y1:y2, x1:x2] = blurred
        
        return Image.fromarray(img_array), (x1, y1, x2, y2)
