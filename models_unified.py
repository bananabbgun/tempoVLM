

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional, Dict, List, Tuple


class UnifiedTempoVLM(nn.Module):
    """
    Unified TempoVLM multi-task model with GRU Long-term Memory

    Tasks:
    - temporal: æ™‚åºä¸€è‡´æ€§ (ä½¿ç”¨ GRU é•·æœŸè¨˜æ†¶)
    - depth_order: æ·±åº¦æ’åº (A vs B èª°æ›´è¿‘)
    - depth_regression: ç›¸å°æ·±åº¦å€¼é æ¸¬
    - motion: ç›¸æ©Ÿé‹å‹•é æ¸¬ (6DoF)
    
    GRU è¨˜æ†¶åŠŸèƒ½:
    - ç¶­è­·é•·æœŸéš±è—ç‹€æ…‹ï¼Œå³ä½¿é€£çºŒå¤šå¹€è¢«é®æ“‹ä¹Ÿèƒ½ä¿ç•™ä¹‹å‰çš„è³‡è¨Š
    - è‡ªå‹•å­¸ç¿’ä½•æ™‚æ›´æ–°/éºå¿˜è¨˜æ†¶
    """
    
    def __init__(
        self,
        feat_dim: int = 1536,
        hidden_dim: int = 768,
        num_scene_classes: int = 20,
        dropout: float = 0.1,
        use_gru_memory: bool = True,  # æ˜¯å¦ä½¿ç”¨ GRU è¨˜æ†¶
    ):
        super().__init__()
        
        self.feat_dim = feat_dim
        self.hidden_dim = hidden_dim
        self.use_gru_memory = use_gru_memory
        
        # ============================================================
        # shared encoder
        # ============================================================
        self.shared_encoder = nn.Sequential(
            nn.Linear(feat_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.GELU(),
            nn.Dropout(dropout),
        )
        
        # ============================================================
        # GRU Long-term Memory (NEW)
        # ============================================================
        if use_gru_memory:
            # GRU Cell: è¼¸å…¥ç•¶å‰è§€æ¸¬ï¼Œè¼¸å‡ºæ›´æ–°å¾Œçš„è¨˜æ†¶
            self.temporal_gru = nn.GRUCell(hidden_dim, hidden_dim)
            
            # è¨˜æ†¶å“è³ªè©•ä¼°å™¨ï¼šè©•ä¼°ç•¶å‰å¹€æ˜¯å¦å¯ä¿¡ï¼ˆç”¨æ–¼æ±ºå®šæ˜¯å¦æ›´æ–°è¨˜æ†¶ï¼‰
            self.memory_quality_gate = nn.Sequential(
                nn.Linear(hidden_dim * 2, hidden_dim),
                nn.GELU(),
                nn.Linear(hidden_dim, 1),
                nn.Sigmoid()
            )
            
            # è¨˜æ†¶èåˆé–€ï¼šæ±ºå®šè¼¸å‡ºæ™‚ä½¿ç”¨å¤šå°‘è¨˜æ†¶ vs ç•¶å‰è§€æ¸¬
            self.memory_output_gate = nn.Sequential(
                nn.Linear(hidden_dim * 2, hidden_dim),
                nn.Sigmoid()
            )
        
        # ============================================================
        # temporal consistency branch (ä¿ç•™åŸæœ‰çµæ§‹ä½œç‚ºå‚™ç”¨)
        # ============================================================
        self.temporal_fusion = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.GELU(),
            nn.Dropout(dropout),
        )
        
        self.temporal_gate = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.Sigmoid()
        )
        
        self.temporal_output = nn.Sequential(
            nn.Linear(hidden_dim, feat_dim),
        )
        
        # ============================================================
        # depth order branch
        # ============================================================
        self.depth_order_head = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, 2),  # [Aè¼ƒè¿‘, Bè¼ƒè¿‘]
        )
        
        # ============================================================
        # depth regression branch (predict absolute depth values for 3 regions)
        # ============================================================
        self.depth_regression_head = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.GELU(),
            nn.Linear(hidden_dim // 2, 3),  # è¼¸å‡º 3 å€‹å€åŸŸ: [left, center, right]
        )
        # æœ€å¤§æ·±åº¦ç¯„åœ (ç”¨æ–¼ sigmoid æ˜ å°„)
        self.max_depth = 10.0  # 10 meters
        
        # ============================================================
        # camera motion prediction branch
        # ============================================================
        self.motion_fusion = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.GELU(),
            nn.Dropout(dropout),
        )
        
        self.motion_head = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.GELU(),
            nn.Linear(hidden_dim // 2, 6),  # [tx, ty, tz, rx, ry, rz]
        )
        # é‹å‹•å°ºåº¦å› å­ (å¯å­¸ç¿’çš„åƒæ•¸) - åˆ†é›¢å¹³ç§»å’Œæ—‹è½‰çš„ scale
        # åˆå§‹åŒ–æ¥è¿‘ ScanNet çš„å…¸å‹é‹å‹•ç¯„åœï¼šå¹³ç§» ~0.01-0.1mï¼Œæ—‹è½‰ ~0.01-0.1rad
        self.motion_scale = nn.Parameter(torch.tensor([0.05, 0.05, 0.05, 0.02, 0.02, 0.02]))
        
        # ============================================================
        # è»Œè·¡ç´¯ç©èª¤å·®ä¿®æ­£æ¨¡çµ„ (NEW)
        # ============================================================
        # 1. Motion Uncertainty Head - é æ¸¬æ¯å¹€é‹å‹•çš„ä¸ç¢ºå®šæ€§
        self.motion_uncertainty_head = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 4),
            nn.GELU(),
            nn.Linear(hidden_dim // 4, 6),  # æ¯å€‹ç¶­åº¦çš„ log variance
        )
        
        # 2. Velocity Consistency - ç”¨æ–¼å¹³æ»‘è»Œè·¡
        self.velocity_smoothing = nn.Sequential(
            nn.Linear(hidden_dim + 6, hidden_dim // 2),  # ç•¶å‰ç‰¹å¾µ + å‰ä¸€å¹€é‹å‹•
            nn.GELU(),
            nn.Linear(hidden_dim // 2, 6),  # ä¿®æ­£é …
        )
        
        # 3. Global Scale Predictor - é æ¸¬å…¨å±€å°ºåº¦å› å­ï¼ˆè§£æ±º scale ä¸ä¸€è‡´å•é¡Œï¼‰
        self.global_scale_head = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 4),
            nn.GELU(),
            nn.Linear(hidden_dim // 4, 1),
            nn.Softplus(),  # ç¢ºä¿ scale > 0
        )
        
        # 4. Motion Quality Detector - æª¢æ¸¬å¿«é€Ÿé‹å‹•/æ¨¡ç³Šå¹€
        self.motion_quality_head = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim // 2),
            nn.GELU(),
            nn.Linear(hidden_dim // 2, 1),
            nn.Sigmoid(),  # 0 = ä½å“è³ª, 1 = é«˜å“è³ª
        )
        
        # 5. Place Recognition - ç°¡åŒ–ç‰ˆ Loop Closureï¼ˆæª¢æ¸¬æ˜¯å¦å›åˆ°ç›¸ä¼¼ä½ç½®ï¼‰
        self.place_embedding = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.LayerNorm(hidden_dim // 2),
        )
       
        self.scene_classifier = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, num_scene_classes),
        )
        
        self._init_weights()
    
    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.LayerNorm):
                nn.init.ones_(m.weight)
                nn.init.zeros_(m.bias)
            elif isinstance(m, nn.GRUCell):
                # GRU ç‰¹æ®Šåˆå§‹åŒ–
                for name, param in m.named_parameters():
                    if 'weight' in name:
                        nn.init.orthogonal_(param)
                    elif 'bias' in name:
                        nn.init.zeros_(param)
    
    def init_hidden_state(self, batch_size: int, device: torch.device) -> torch.Tensor:
        """åˆå§‹åŒ– GRU éš±è—ç‹€æ…‹"""
        return torch.zeros(batch_size, self.hidden_dim, device=device)
    
    def load_pretrained_temporal(self, checkpoint_path: str, strict: bool = False):

        checkpoint = torch.load(checkpoint_path, map_location='cpu')
        state_dict = checkpoint.get('model_state_dict', checkpoint)
        
        print(f"ğŸ“¦ åŸå§‹ checkpoint åŒ…å«çš„ keys:")
        for k, v in state_dict.items():
            print(f"   {k}: {v.shape}")
        
        compatible_keys = []
        incompatible_keys = []
        
        
        if 'gate.0.weight' in state_dict:
            old_weight = state_dict['gate.0.weight']  # [768, 3072]
            new_weight_shape = self.temporal_gate[0].weight.shape  # [768, 1536]
            
            if old_weight.shape[0] == new_weight_shape[0]:
            
                self.temporal_gate[0].weight.data = old_weight[:, :new_weight_shape[1]].clone()
                if 'gate.0.bias' in state_dict:
                    self.temporal_gate[0].bias.data = state_dict['gate.0.bias'].clone()
                compatible_keys.append('gate.0 (partial)')
        
        if 'refine.0.weight' in state_dict:
            old_shape = state_dict['refine.0.weight'].shape
            new_shape = self.temporal_output[0].weight.shape
            
            if old_shape == new_shape:
                self.temporal_output[0].weight.data = state_dict['refine.0.weight'].clone()
                self.temporal_output[0].bias.data = state_dict['refine.0.bias'].clone()
                compatible_keys.append('refine.0')
        
        print(f"\nâœ… é è¨“ç·´æ¬Šé‡è¼‰å…¥çµæœ:")
        print(f"   - éƒ¨åˆ†ç›¸å®¹: {compatible_keys}")
        print(f"   - çµæ§‹ä¸åŒï¼Œéœ€é‡æ–°è¨“ç·´: shared_encoder, temporal_fusion")
        print(f"   âš ï¸ ç”±æ–¼æ¶æ§‹å·®ç•°ï¼Œå»ºè­°é‡æ–°è¨“ç·´æˆ–ä½¿ç”¨ --no_pretrained")
        
        return compatible_keys
    
    def forward(
        self,
        curr_feat: torch.Tensor,
        prev_feat: Optional[torch.Tensor] = None,
        hidden_state: Optional[torch.Tensor] = None,  # GRU éš±è—ç‹€æ…‹ (NEW)
        region_a_feat: Optional[torch.Tensor] = None,
        region_b_feat: Optional[torch.Tensor] = None,
        tasks: List[str] = ['temporal'],
    ) -> Tuple[Dict[str, torch.Tensor], Optional[torch.Tensor]]:
        """
        Forward pass with optional GRU memory
        
        Args:
            curr_feat: ç•¶å‰å¹€ç‰¹å¾µ [B, feat_dim]
            prev_feat: å‰ä¸€å¹€ç‰¹å¾µ [B, feat_dim] (temporal/motion ç”¨)
            hidden_state: GRU éš±è—ç‹€æ…‹ [B, hidden_dim] (é•·æœŸè¨˜æ†¶)
            region_a_feat: å€åŸŸ A ç‰¹å¾µ [B, feat_dim] (depth_order ç”¨)
            region_b_feat: å€åŸŸ B ç‰¹å¾µ [B, feat_dim] (depth_order ç”¨)
            tasks: è¦åŸ·è¡Œçš„ä»»å‹™åˆ—è¡¨
        
        Returns:
            outputs: åŒ…å«å„ä»»å‹™è¼¸å‡ºçš„å­—å…¸
            next_hidden_state: æ›´æ–°å¾Œçš„ GRU éš±è—ç‹€æ…‹ (å¦‚æœä½¿ç”¨ GRU)
        """
        outputs = {}
        next_hidden_state = None
        
        # ç·¨ç¢¼ç•¶å‰å¹€
        curr_enc = self.shared_encoder(curr_feat)  # [B, hidden_dim]
        batch_size = curr_feat.shape[0]
        device = curr_feat.device
        
        # ============================================================
        # Task 1 : temporal consistency (with GRU Long-term Memory)
        # ============================================================
        if 'temporal' in tasks:
            if self.use_gru_memory:
                # ========== GRU é•·æœŸè¨˜æ†¶æ¨¡å¼ ==========
                
                # åˆå§‹åŒ–éš±è—ç‹€æ…‹ï¼ˆå¦‚æœæ˜¯ç¬¬ä¸€å¹€æˆ–æ–°å ´æ™¯ï¼‰
                if hidden_state is None:
                    hidden_state = self.init_hidden_state(batch_size, device)
                
                # 1. è©•ä¼°ç•¶å‰å¹€çš„å“è³ªï¼ˆæ˜¯å¦è¢«é®æ“‹ï¼‰
                #    æ¯”è¼ƒç•¶å‰è§€æ¸¬å’Œé•·æœŸè¨˜æ†¶çš„å·®ç•°
                combined_for_quality = torch.cat([curr_enc, hidden_state], dim=-1)
                quality_score = self.memory_quality_gate(combined_for_quality)  # [B, 1]
                
                # 2. GRU æ›´æ–°è¨˜æ†¶
                #    quality_score é«˜ = ç•¶å‰å¹€å¯ä¿¡ï¼Œå¤šæ›´æ–°è¨˜æ†¶
                #    quality_score ä½ = ç•¶å‰å¹€å¯èƒ½è¢«é®æ“‹ï¼Œå°‘æ›´æ–°è¨˜æ†¶
                gru_input = curr_enc * quality_score + hidden_state * (1 - quality_score)
                new_memory = self.temporal_gru(gru_input, hidden_state)
                
                # 3. æ±ºå®šè¼¸å‡ºæ™‚ä½¿ç”¨å¤šå°‘è¨˜æ†¶
                combined_for_output = torch.cat([curr_enc, new_memory], dim=-1)
                output_gate = self.memory_output_gate(combined_for_output)  # [B, hidden_dim]
                
                # 4. èåˆç•¶å‰è§€æ¸¬å’Œé•·æœŸè¨˜æ†¶
                fused_enc = output_gate * new_memory + (1 - output_gate) * curr_enc
                
                # 5. è¼¸å‡ºç²¾ç…‰å¾Œçš„ç‰¹å¾µ
                refined = self.temporal_output(fused_enc)
                
                # 6. æ®˜å·®é€£æ¥
                outputs['temporal'] = curr_feat + refined
                outputs['temporal_gate'] = output_gate.mean()
                outputs['memory_quality'] = quality_score.mean()  # ç”¨æ–¼ç›£æ§
                
                # æ›´æ–°éš±è—ç‹€æ…‹
                next_hidden_state = new_memory
                
            elif prev_feat is not None:
                # ========== åŸå§‹æ¨¡å¼ï¼ˆç„¡ GRUï¼‰==========
                prev_enc = self.shared_encoder(prev_feat)
                
                # fusion
                combined = torch.cat([curr_enc, prev_enc], dim=-1)
                fused = self.temporal_fusion(combined)
                
                # gate
                gate = self.temporal_gate(combined)
                gated = fused * gate + curr_enc * (1 - gate)

                # output refined features
                refined = self.temporal_output(gated)
                
                # residual connection
                outputs['temporal'] = curr_feat + refined
                outputs['temporal_gate'] = gate.mean() 
        
        # ============================================================
        # Task 2 : depth order
        # ============================================================
        if 'depth_order' in tasks:
            if region_a_feat is not None and region_b_feat is not None:
                region_a_enc = self.shared_encoder(region_a_feat)
                region_b_enc = self.shared_encoder(region_b_feat)
                combined = torch.cat([region_a_enc, region_b_enc], dim=-1)
                outputs['depth_order'] = self.depth_order_head(combined)  # [B, 2]
            else:
                # ä½¿ç”¨å…¨åœ–ç‰¹å¾µçš„ä¸åŒå€åŸŸï¼ˆç°¡åŒ–ç‰ˆï¼‰
                outputs['depth_order'] = None
        
        # ============================================================
        # Task 3 : depth regression (è¼¸å‡º 3 å€‹å€åŸŸçš„çµ•å°æ·±åº¦)
        # ============================================================
        if 'depth_regression' in tasks:
            raw_depth = self.depth_regression_head(curr_enc)  # [B, 3]
            # ä½¿ç”¨ softplus ç¢ºä¿è¼¸å‡ºç‚ºæ­£æ•¸ï¼Œä¸¦é™åˆ¶åœ¨åˆç†ç¯„åœå…§
            # softplus(x) = log(1 + exp(x))ï¼Œå¹³æ»‘çš„ ReLU
            depth = F.softplus(raw_depth) * (self.max_depth / 5.0)  # scale to ~0-10m range
            # æˆ–è€…ç”¨ sigmoid: depth = torch.sigmoid(raw_depth) * self.max_depth
            outputs['depth_regression'] = depth  # [B, 3] = [left, center, right]
        
        # ============================================================
        # Task 4 : camera motion prediction (with quality & scale correction)
        # ============================================================
        if 'motion' in tasks and prev_feat is not None:
            prev_enc = self.shared_encoder(prev_feat)
            combined = torch.cat([curr_enc, prev_enc], dim=-1)
            fused = self.motion_fusion(combined)
            raw_motion = self.motion_head(fused)  # [B, 6]
            
            # 1. åŸºç¤é‹å‹•é æ¸¬ + scale åƒæ•¸
            motion = raw_motion * self.motion_scale.unsqueeze(0)  # [B, 6]
            
            # 2. é æ¸¬é‹å‹•ä¸ç¢ºå®šæ€§ (ç”¨æ–¼åŠ æ¬Š loss)
            motion_log_var = self.motion_uncertainty_head(fused)  # [B, 6]
            motion_uncertainty = torch.exp(motion_log_var)  # [B, 6]
            
            # 3. é æ¸¬å…¨å±€ scale factor (ç”¨æ–¼æ ¡æ­£ç´¯ç©èª¤å·®)
            global_scale = self.global_scale_head(curr_enc)  # [B, 1]
            # å°‡ global_scale é™åˆ¶åœ¨åˆç†ç¯„åœ [0.5, 2.0]
            global_scale = 0.5 + 1.5 * torch.sigmoid(global_scale - 1)
            
            # 4. æª¢æ¸¬é‹å‹•å“è³ªï¼ˆå¿«é€Ÿé‹å‹•/æ¨¡ç³Šæª¢æ¸¬ï¼‰
            motion_quality = self.motion_quality_head(combined)  # [B, 1]
            
            # 5. Place Recognition embedding (ç”¨æ–¼ Loop Closure)
            place_emb = self.place_embedding(curr_enc)  # [B, hidden_dim//2]
            
            outputs['motion'] = motion
            outputs['motion_raw'] = raw_motion  # åŸå§‹é æ¸¬ï¼ˆç”¨æ–¼åˆ†æï¼‰
            outputs['motion_uncertainty'] = motion_uncertainty
            outputs['motion_log_var'] = motion_log_var
            outputs['motion_global_scale'] = global_scale
            outputs['motion_quality'] = motion_quality
            outputs['place_embedding'] = place_emb
        
        if 'scene_class' in tasks:
            outputs['scene_class'] = self.scene_classifier(curr_enc)  # [B, num_classes]
        
        # è¿”å› outputs å’Œ next_hidden_stateï¼ˆå¦‚æœä½¿ç”¨ GRU è¨˜æ†¶ï¼‰
        if self.use_gru_memory and 'temporal' in tasks:
            return outputs, next_hidden_state
        else:
            return outputs, None
    
    def forward_temporal(
        self, 
        curr_feat: torch.Tensor, 
        prev_feat: torch.Tensor = None,
        hidden_state: torch.Tensor = None
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:
        """ä¾¿åˆ©æ–¹æ³•ï¼šåªåŸ·è¡Œ temporal ä»»å‹™"""
        outputs, next_hidden = self.forward(
            curr_feat, prev_feat, 
            hidden_state=hidden_state,
            tasks=['temporal']
        )
        return outputs['temporal'], next_hidden
    
    def forward_depth_order(
        self, 
        region_a_feat: torch.Tensor, 
        region_b_feat: torch.Tensor
    ) -> torch.Tensor:
        outputs, _ = self.forward(
            region_a_feat, 
            region_a_feat=region_a_feat,
            region_b_feat=region_b_feat,
            tasks=['depth_order']
        )
        return outputs['depth_order']
    
    def forward_motion(
        self, 
        curr_feat: torch.Tensor, 
        prev_feat: torch.Tensor
    ) -> torch.Tensor:
        outputs, _ = self.forward(curr_feat, prev_feat, tasks=['motion'])
        return outputs['motion']


class UnifiedLoss(nn.Module):
    """
    Unified multi-task loss for UnifiedTempoVLM
    ä½¿ç”¨ Kendall et al. (CVPR 2018) çš„è‡ªå‹•åŠ æ¬Šæ–¹æ³•
    """
    def __init__(
        self,
        num_tasks: int = 5,
        use_uncertainty_weighting: bool = True,
    ):
        super().__init__()
        
        self.use_uncertainty_weighting = use_uncertainty_weighting
        
        # å¯å­¸ç¿’çš„ log variance åƒæ•¸ (ç”¨æ–¼è‡ªå‹• Loss å¹³è¡¡)
        # ä»»å‹™é †åº: [temporal, depth_order, depth_regression, motion, scene_class]
        if use_uncertainty_weighting:
            self.log_vars = nn.Parameter(torch.zeros(num_tasks))
        else:
            # å›ºå®šæ¬Šé‡ (å‚™ç”¨)
            self.register_buffer('fixed_weights', torch.ones(num_tasks))
        
        self.ce_loss = nn.CrossEntropyLoss()
        self.mse_loss = nn.MSELoss()
    
    def _weighted_loss(self, loss, task_idx):
        """
        æ ¹æ“šä¸ç¢ºå®šæ€§è‡ªå‹•åŠ æ¬Š loss
        å…¬å¼: L_weighted = L / (2 * ÏƒÂ²) + log(Ïƒ) = L * exp(-log_var) / 2 + log_var / 2
        """
        if self.use_uncertainty_weighting:
            # Clamp log_vars é¿å…æ•¸å€¼ä¸ç©©å®š
            log_var = torch.clamp(self.log_vars[task_idx], min=-4, max=4)
            precision = torch.exp(-log_var)
            return precision * loss + 0.5 * log_var
        else:
            return loss * self.fixed_weights[task_idx]
    
    def scale_invariant_depth_loss(self, pred, target):
        """
        Scale-Invariant Loss for depth prediction
        å°æ·±åº¦é æ¸¬æ›´ç©©å®šï¼Œä¸å—çµ•å°å°ºåº¦å½±éŸ¿
        """
        # å‰µå»ºæœ‰æ•ˆæ·±åº¦çš„ maskï¼ˆæ’é™¤ç„¡æ•ˆçš„ 0 å€¼å€åŸŸï¼‰
        valid_mask = target > 0.1  # åªè€ƒæ…®æ·±åº¦ > 0.1m çš„å€åŸŸ
        
        if valid_mask.sum() == 0:
            # æ²’æœ‰æœ‰æ•ˆæ·±åº¦ï¼Œè¿”å› 0 loss
            return torch.tensor(0.0, device=pred.device, requires_grad=True)
        
        # åªè¨ˆç®—æœ‰æ•ˆå€åŸŸ
        pred_valid = pred[valid_mask].clamp(min=1e-6)
        target_valid = target[valid_mask].clamp(min=1e-6)
        
        # å°æ•¸ç©ºé–“çš„å·®ç•°
        log_diff = torch.log(pred_valid) - torch.log(target_valid)
        
        # Scale-invariant loss
        n = log_diff.numel()
        if n == 0:
            return torch.tensor(0.0, device=pred.device, requires_grad=True)
            
        si_loss = torch.sum(log_diff ** 2) / n - (torch.sum(log_diff) ** 2) / (n ** 2)
        
        # åŠ ä¸Š L1 loss ä½œç‚ºæ­£å‰‡åŒ–
        l1_loss = F.l1_loss(pred_valid, target_valid)
        
        return si_loss + 0.5 * l1_loss
    
    def motion_loss(self, pred, target, log_var=None):
        """
        é‹å‹•é æ¸¬çš„ lossï¼Œåˆ†åˆ¥è™•ç†å¹³ç§»å’Œæ—‹è½‰
        æ”¯æ´ä¸ç¢ºå®šæ€§åŠ æ¬Š (Learned Uncertainty)
        
        Args:
            pred: é æ¸¬é‹å‹• [B, 6]
            target: GT é‹å‹• [B, 6]
            log_var: é æ¸¬çš„ log variance [B, 6] (å¯é¸)
        """
        # åˆ†é›¢å¹³ç§» [tx, ty, tz] å’Œæ—‹è½‰ [rx, ry, rz]
        pred_trans = pred[:, :3]
        pred_rot = pred[:, 3:]
        target_trans = target[:, :3]
        target_rot = target[:, 3:]
        
        if log_var is not None:
            # æ–¹æ¡ˆï¼šä½¿ç”¨è»Ÿæ€§ä¸ç¢ºå®šæ€§åŠ æ¬Šï¼ˆä¸æœƒç”¢ç”Ÿè²  lossï¼‰
            # ä½¿ç”¨ softplus ç¢ºä¿ variance > 0
            # L = |pred - target|^2 / (2 * variance) + 0.5 * log(variance)
            # å…¶ä¸­ variance = softplus(log_var) + 1e-6
            
            # å°‡ log_var è½‰ç‚ºæ­£çš„ variance
            variance = F.softplus(log_var) + 1e-6  # [B, 6], ç¢ºä¿ > 0
            
            var_trans = variance[:, :3]
            var_rot = variance[:, 3:]
            
            # å¹³ç§» lossï¼ˆèª¤å·® / variance + log(variance)ï¼‰
            trans_error = (pred_trans - target_trans) ** 2
            trans_loss = (trans_error / (2 * var_trans)).mean() + 0.5 * torch.log(var_trans).mean()
            
            # æ—‹è½‰ loss
            rot_error = (pred_rot - target_rot) ** 2
            rot_loss = (rot_error / (2 * var_rot)).mean() + 0.5 * torch.log(var_rot).mean()
            
            # åŠ ä¸€å€‹ baseline loss ç¢ºä¿æœ‰æ¢¯åº¦
            baseline_trans = F.smooth_l1_loss(pred_trans, target_trans)
            baseline_rot = F.mse_loss(pred_rot, target_rot)
            
            # æ··åˆï¼š50% ä¸ç¢ºå®šæ€§åŠ æ¬Š + 50% baseline
            trans_loss = 0.5 * trans_loss + 0.5 * baseline_trans
            rot_loss = 0.5 * rot_loss + 0.5 * baseline_rot
            
        else:
            # åŸå§‹ loss
            trans_loss = F.smooth_l1_loss(pred_trans, target_trans)
            rot_loss = F.mse_loss(pred_rot, target_rot)
        
        # åŠ å…¥é€Ÿåº¦ä¸€è‡´æ€§æ­£å‰‡åŒ–
        # é¼“å‹µç›¸é„°é æ¸¬çš„è®ŠåŒ–å¹³æ»‘
        if pred.shape[0] > 1:
            velocity_diff = pred[1:] - pred[:-1]
            smoothness_loss = 0.01 * (velocity_diff ** 2).mean()
        else:
            smoothness_loss = 0
        
        return trans_loss + rot_loss + smoothness_loss
    
    def forward(
        self,
        outputs: Dict[str, torch.Tensor],
        targets: Dict[str, torch.Tensor],
        prev_feat: Optional[torch.Tensor] = None,
    ) -> Tuple[torch.Tensor, Dict[str, float]]:
        """
        calculate unified multi-task loss with automatic weighting
        
        Args:
            outputs: model output dictionary
            targets: target dictionary
            prev_feat: previous frame features (for temporal consistency)
        
        Returns:
            total_loss: total loss
            loss_dict: individual task loss dictionary (åŒ…å«åŸå§‹ loss å’Œæ¬Šé‡)
        """
        total_loss = 0
        loss_dict = {}
        
        # Task 0: temporal consistency loss
        if 'temporal' in outputs and prev_feat is not None:
            refined = outputs['temporal']
            # consistency with previous frame
            temporal_loss = 1 - F.cosine_similarity(
                refined.float(), prev_feat.float(), dim=-1
            ).mean()
            weighted_loss = self._weighted_loss(temporal_loss, task_idx=0)
            total_loss += weighted_loss
            loss_dict['temporal'] = temporal_loss.item()
            if self.use_uncertainty_weighting:
                loss_dict['temporal_weight'] = torch.exp(-self.log_vars[0]).item()
        
        # Task 1: depth order loss
        if 'depth_order' in outputs and outputs['depth_order'] is not None:
            if 'depth_order' in targets:
                depth_order_loss = self.ce_loss(
                    outputs['depth_order'],
                    targets['depth_order']
                )
                weighted_loss = self._weighted_loss(depth_order_loss, task_idx=1)
                total_loss += weighted_loss
                loss_dict['depth_order'] = depth_order_loss.item()
                if self.use_uncertainty_weighting:
                    loss_dict['depth_order_weight'] = torch.exp(-self.log_vars[1]).item()
        
        # Task 2: depth regression loss (ä½¿ç”¨ Scale-Invariant Loss)
        if 'depth_regression' in outputs and outputs['depth_regression'] is not None:
            if 'depth_regression' in targets:
                pred_depth = outputs['depth_regression']  # [B, 3]
                target_depth = targets['depth_regression']  # [B, 3] or [B, 1]
                
                # å¦‚æœ target åªæœ‰ 1 ç¶­ï¼Œæ“´å±•åˆ° 3 ç¶­
                if target_depth.dim() == 1:
                    target_depth = target_depth.unsqueeze(-1).expand(-1, 3)
                elif target_depth.shape[-1] == 1:
                    target_depth = target_depth.expand(-1, 3)
                
                depth_reg_loss = self.scale_invariant_depth_loss(
                    pred_depth, target_depth
                )
                weighted_loss = self._weighted_loss(depth_reg_loss, task_idx=2)
                total_loss += weighted_loss
                loss_dict['depth_regression'] = depth_reg_loss.item()
                if self.use_uncertainty_weighting:
                    loss_dict['depth_regression_weight'] = torch.exp(-self.log_vars[2]).item()
        
        # Task 3: motion prediction loss (åˆ†é›¢å¹³ç§»å’Œæ—‹è½‰ï¼Œæ”¯æ´ä¸ç¢ºå®šæ€§)
        if 'motion' in outputs and 'motion' in targets:
            # ä½¿ç”¨é æ¸¬çš„ä¸ç¢ºå®šæ€§ï¼ˆå¦‚æœæœ‰çš„è©±ï¼‰
            motion_log_var = outputs.get('motion_log_var', None)
            
            motion_loss = self.motion_loss(
                outputs['motion'],
                targets['motion'],
                log_var=motion_log_var
            )
            weighted_loss = self._weighted_loss(motion_loss, task_idx=3)
            total_loss += weighted_loss
            loss_dict['motion'] = motion_loss.item()
            if self.use_uncertainty_weighting:
                loss_dict['motion_weight'] = torch.exp(-self.log_vars[3]).item()
            
            # è¨˜éŒ„å¹³å‡ä¸ç¢ºå®šæ€§ï¼ˆç”¨æ–¼ç›£æ§ï¼‰
            if motion_log_var is not None:
                loss_dict['motion_avg_uncertainty'] = torch.exp(motion_log_var).mean().item()
            
            # Motion Quality ç›£ç£ï¼ˆå¦‚æœæœ‰æ¨™ç±¤ï¼‰
            if 'motion_quality' in outputs and 'motion_quality_label' in targets:
                quality_loss = F.binary_cross_entropy(
                    outputs['motion_quality'],
                    targets['motion_quality_label']
                )
                total_loss += 0.1 * quality_loss  # å°æ¬Šé‡
                loss_dict['motion_quality'] = quality_loss.item()
            
            # Global Scale ç›£ç£ï¼ˆå¦‚æœæœ‰æ¨™ç±¤ï¼‰
            if 'motion_global_scale' in outputs and 'motion_scale_label' in targets:
                scale_loss = F.mse_loss(
                    outputs['motion_global_scale'],
                    targets['motion_scale_label']
                )
                total_loss += 0.1 * scale_loss
                loss_dict['scale'] = scale_loss.item()
        
        # Task 4: scene classification loss
        if 'scene_class' in outputs and 'scene_class' in targets:
            scene_loss = self.ce_loss(
                outputs['scene_class'],
                targets['scene_class']
            )
            weighted_loss = self._weighted_loss(scene_loss, task_idx=4)
            total_loss += weighted_loss
            loss_dict['scene_class'] = scene_loss.item()
            if self.use_uncertainty_weighting:
                loss_dict['scene_class_weight'] = torch.exp(-self.log_vars[4]).item()
        
        return total_loss, loss_dict
    
    def get_task_weights(self) -> Dict[str, float]:
        """å–å¾—ç•¶å‰å„ä»»å‹™çš„è‡ªå‹•æ¬Šé‡ (ç”¨æ–¼ç›£æ§)"""
        if self.use_uncertainty_weighting:
            task_names = ['temporal', 'depth_order', 'depth_regression', 'motion', 'scene_class']
            weights = torch.exp(-self.log_vars).detach().cpu().numpy()
            return {name: float(w) for name, w in zip(task_names, weights)}
        else:
            return {}


# ============================================================
# tools
# ============================================================

def create_unified_model(
    feat_dim: int = 1536,
    pretrained_temporal_path: Optional[str] = None,
) -> UnifiedTempoVLM:

    model = UnifiedTempoVLM(feat_dim=feat_dim)
    
    if pretrained_temporal_path:
        model.load_pretrained_temporal(pretrained_temporal_path)
    
    return model


def get_model_info(model: UnifiedTempoVLM) -> Dict:
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    # å„åˆ†æ”¯åƒæ•¸é‡
    branch_params = {
        'shared_encoder': sum(p.numel() for p in model.shared_encoder.parameters()),
        'temporal': sum(p.numel() for n, p in model.named_parameters() if 'temporal' in n and 'gru' not in n.lower()),
        'depth_order': sum(p.numel() for p in model.depth_order_head.parameters()),
        'depth_regression': sum(p.numel() for p in model.depth_regression_head.parameters()),
        'motion': sum(p.numel() for n, p in model.named_parameters() if 'motion' in n),
        'scene_classifier': sum(p.numel() for p in model.scene_classifier.parameters()),
    }
    
    # GRU è¨˜æ†¶ç›¸é—œåƒæ•¸
    if model.use_gru_memory:
        gru_params = sum(p.numel() for p in model.temporal_gru.parameters())
        memory_gate_params = sum(p.numel() for p in model.memory_quality_gate.parameters())
        memory_gate_params += sum(p.numel() for p in model.memory_output_gate.parameters())
        branch_params['gru_memory'] = gru_params + memory_gate_params
    
    return {
        'total_params': total_params,
        'trainable_params': trainable_params,
        'branch_params': branch_params,
    }


# ============================================================
# Testing
# ============================================================

if __name__ == "__main__":
    print("Testing UnifiedTempoVLM...")

    # æ¸¬è©¦ç„¡ GRU æ¨¡å¼
    print("\n========== æ¸¬è©¦åŸºæœ¬æ¨¡å¼ï¼ˆç„¡ GRUï¼‰==========")
    model = UnifiedTempoVLM(feat_dim=1536, hidden_dim=768, use_gru_memory=False)
    model.eval()
    
    batch_size = 2
    curr_feat = torch.randn(batch_size, 1536)
    prev_feat = torch.randn(batch_size, 1536)
    region_a = torch.randn(batch_size, 1536)
    region_b = torch.randn(batch_size, 1536)
    
    print("\nTesting multi-task forward propagation...")
    outputs, hidden = model(
        curr_feat=curr_feat,
        prev_feat=prev_feat,
        region_a_feat=region_a,
        region_b_feat=region_b,
        tasks=['temporal', 'depth_order', 'depth_regression', 'motion', 'scene_class']
    )
    
    print(f"  temporal output: {outputs['temporal'].shape}")
    print(f"  depth_order output: {outputs['depth_order'].shape}")
    print(f"  depth_regression output: {outputs['depth_regression'].shape}")  # æ‡‰è©²æ˜¯ [B, 3]
    print(f"  depth_regression values: {outputs['depth_regression']}")  # æª¢æŸ¥æ˜¯å¦ç‚ºæ­£æ•¸
    print(f"  motion output: {outputs['motion'].shape}")
    print(f"  motion values: {outputs['motion']}")  # æª¢æŸ¥é‹å‹•å€¼
    print(f"  scene_class output: {outputs['scene_class'].shape}")
    print(f"  next_hidden_state: {hidden}")  # æ‡‰è©²æ˜¯ Noneï¼ˆç„¡ GRU æ¨¡å¼ï¼‰

    # æ¸¬è©¦ GRU è¨˜æ†¶æ¨¡å¼
    print("\n========== æ¸¬è©¦ GRU è¨˜æ†¶æ¨¡å¼ ==========")
    model_gru = UnifiedTempoVLM(feat_dim=1536, hidden_dim=768, use_gru_memory=True)
    model_gru.eval()
    
    print("\næ¨¡æ“¬é€£çºŒå¹€è™•ç†...")
    hidden_state = None
    for frame_idx in range(5):
        curr_feat = torch.randn(batch_size, 1536)
        outputs, hidden_state = model_gru(
            curr_feat=curr_feat,
            hidden_state=hidden_state,
            tasks=['temporal']
        )
        print(f"  Frame {frame_idx}: temporal_gate={outputs.get('temporal_gate', 'N/A'):.3f}, "
              f"memory_quality={outputs.get('memory_quality', 'N/A'):.3f}, "
              f"hidden_state shape={hidden_state.shape if hidden_state is not None else 'None'}")

    print("\nTesting loss calculation with automatic weighting...")
    loss_fn = UnifiedLoss(use_uncertainty_weighting=True)
    targets = {
        'depth_order': torch.randint(0, 2, (batch_size,)),
        'depth_regression': torch.rand(batch_size, 3) * 5 + 0.5,  # 0.5~5.5m çš„æ·±åº¦
        'motion': torch.randn(batch_size, 6) * 0.1,  # å°çš„é‹å‹•å€¼
        'scene_class': torch.randint(0, 20, (batch_size,)),
    }
    
    # ä½¿ç”¨ç„¡ GRU æ¨¡å‹çš„ outputs
    outputs, _ = model(
        curr_feat=torch.randn(batch_size, 1536),
        prev_feat=torch.randn(batch_size, 1536),
        region_a_feat=region_a,
        region_b_feat=region_b,
        tasks=['temporal', 'depth_order', 'depth_regression', 'motion', 'scene_class']
    )
    
    total_loss, loss_dict = loss_fn(outputs, targets, prev_feat)
    print(f"  Total loss: {total_loss.item():.4f}")
    print(f"  Individual losses:")
    for k, v in loss_dict.items():
        if not k.endswith('_weight'):
            print(f"    {k}: {v:.4f}")
    
    print(f"\n  Auto-learned task weights (åˆå§‹æ‡‰è©²éƒ½æ¥è¿‘ 1.0):")
    weights = loss_fn.get_task_weights()
    for task, weight in weights.items():
        print(f"    {task}: {weight:.4f}")
    
    print(f"\n  Log variance parameters:")
    for i, name in enumerate(['temporal', 'depth_order', 'depth_regression', 'motion', 'scene_class']):
        print(f"    {name}: log_var = {loss_fn.log_vars[i].item():.4f}")

    print("\nModel Information:")
    info = get_model_info(model)
    print(f"  Total Parameters: {info['total_params']:,}")
    print(f"  Branch Parameters:")
    for branch, params in info['branch_params'].items():
        print(f"    {branch}: {params:,}")
    
    info_gru = get_model_info(model_gru)
    print(f"\n  GRU Model Total Parameters: {info_gru['total_params']:,}")
    print(f"  GRU Branch Parameters:")
    for branch, params in info_gru['branch_params'].items():
        print(f"    {branch}: {params:,}")
    
    # æ¸¬è©¦ Loss å‡½æ•¸çš„åƒæ•¸é‡
    loss_params = sum(p.numel() for p in loss_fn.parameters())
    print(f"\n  Loss function learnable params: {loss_params}")

    print("\nâœ… Testing completed!")
